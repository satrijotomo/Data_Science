{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "0         -0.923527         -0.934724    ...      \n",
       "1         -0.957686         -0.943068    ...      \n",
       "2         -0.977469         -0.938692    ...      \n",
       "3         -0.989302         -0.938692    ...      \n",
       "4         -0.990441         -0.942469    ...      \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \n",
       "0              0.179941             -0.058627        1  STANDING  \n",
       "1              0.180289             -0.054317        1  STANDING  \n",
       "2              0.180637             -0.049118        1  STANDING  \n",
       "3              0.181935             -0.047663        1  STANDING  \n",
       "4              0.185151             -0.043892        1  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257178</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>-0.014654</td>\n",
       "      <td>-0.938404</td>\n",
       "      <td>-0.920091</td>\n",
       "      <td>-0.667683</td>\n",
       "      <td>-0.952501</td>\n",
       "      <td>-0.925249</td>\n",
       "      <td>-0.674302</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705974</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>-0.825886</td>\n",
       "      <td>0.271151</td>\n",
       "      <td>-0.720009</td>\n",
       "      <td>0.276801</td>\n",
       "      <td>-0.057978</td>\n",
       "      <td>2</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286027</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>-0.119083</td>\n",
       "      <td>-0.975415</td>\n",
       "      <td>-0.967458</td>\n",
       "      <td>-0.944958</td>\n",
       "      <td>-0.986799</td>\n",
       "      <td>-0.968401</td>\n",
       "      <td>-0.945823</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594944</td>\n",
       "      <td>-0.083495</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>-0.434375</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>-0.698091</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>-0.083898</td>\n",
       "      <td>2</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275485</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>-0.118152</td>\n",
       "      <td>-0.993819</td>\n",
       "      <td>-0.969926</td>\n",
       "      <td>-0.962748</td>\n",
       "      <td>-0.994403</td>\n",
       "      <td>-0.970735</td>\n",
       "      <td>-0.963483</td>\n",
       "      <td>-0.939260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640736</td>\n",
       "      <td>-0.034956</td>\n",
       "      <td>0.202302</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>-0.702771</td>\n",
       "      <td>0.280083</td>\n",
       "      <td>-0.079346</td>\n",
       "      <td>2</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270298</td>\n",
       "      <td>-0.032614</td>\n",
       "      <td>-0.117520</td>\n",
       "      <td>-0.994743</td>\n",
       "      <td>-0.973268</td>\n",
       "      <td>-0.967091</td>\n",
       "      <td>-0.995274</td>\n",
       "      <td>-0.974471</td>\n",
       "      <td>-0.968897</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.017067</td>\n",
       "      <td>0.154438</td>\n",
       "      <td>0.340134</td>\n",
       "      <td>0.296407</td>\n",
       "      <td>-0.698954</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>-0.077108</td>\n",
       "      <td>2</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274833</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.129527</td>\n",
       "      <td>-0.993852</td>\n",
       "      <td>-0.967445</td>\n",
       "      <td>-0.978295</td>\n",
       "      <td>-0.994111</td>\n",
       "      <td>-0.965953</td>\n",
       "      <td>-0.977346</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846595</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.040046</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>-0.118545</td>\n",
       "      <td>-0.692245</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>-0.073857</td>\n",
       "      <td>2</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.257178          -0.023285          -0.014654         -0.938404   \n",
       "1           0.286027          -0.013163          -0.119083         -0.975415   \n",
       "2           0.275485          -0.026050          -0.118152         -0.993819   \n",
       "3           0.270298          -0.032614          -0.117520         -0.994743   \n",
       "4           0.274833          -0.027848          -0.129527         -0.993852   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.920091         -0.667683         -0.952501         -0.925249   \n",
       "1         -0.967458         -0.944958         -0.986799         -0.968401   \n",
       "2         -0.969926         -0.962748         -0.994403         -0.970735   \n",
       "3         -0.973268         -0.967091         -0.995274         -0.974471   \n",
       "4         -0.967445         -0.978295         -0.994111         -0.965953   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "0         -0.674302         -0.894088    ...      \n",
       "1         -0.945823         -0.894088    ...      \n",
       "2         -0.963483         -0.939260    ...      \n",
       "3         -0.968897         -0.938610    ...      \n",
       "4         -0.977346         -0.938610    ...      \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.705974                     0.006462   \n",
       "1                        -0.594944                    -0.083495   \n",
       "2                        -0.640736                    -0.034956   \n",
       "3                        -0.736124                    -0.017067   \n",
       "4                        -0.846595                    -0.002223   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.162920                         -0.825886   \n",
       "1                              0.017500                         -0.434375   \n",
       "2                              0.202302                          0.064103   \n",
       "3                              0.154438                          0.340134   \n",
       "4                             -0.040046                          0.736715   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                              0.271151             -0.720009   \n",
       "1                              0.920593             -0.698091   \n",
       "2                              0.145068             -0.702771   \n",
       "3                              0.296407             -0.698954   \n",
       "4                             -0.118545             -0.692245   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \n",
       "0              0.276801             -0.057978        2  STANDING  \n",
       "1              0.281343             -0.083898        2  STANDING  \n",
       "2              0.280083             -0.079346        2  STANDING  \n",
       "3              0.284114             -0.077108        2  STANDING  \n",
       "4              0.290722             -0.073857        2  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: (999, 563)\n",
      "Test Data shape:  (999, 563)\n"
     ]
    }
   ],
   "source": [
    "print('Train Data shape:', train.shape)\n",
    "print('Test Data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               209\n",
       "STANDING              179\n",
       "LAYING                164\n",
       "WALKING_UPSTAIRS      159\n",
       "WALKING_DOWNSTAIRS    145\n",
       "SITTING               143\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               185\n",
       "LAYING                183\n",
       "STANDING              178\n",
       "SITTING               170\n",
       "WALKING_UPSTAIRS      149\n",
       "WALKING_DOWNSTAIRS    134\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is equally distributed and we don't see any skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# suffling data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "test = shuffle(test)\n",
    "train = shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separating data inputs and outputs labels\n",
    "\n",
    "testData = test.drop(['Activity', 'subject'], axis = 1).values\n",
    "testLabel = test.Activity.values\n",
    "\n",
    "trainData = train.drop(['Activity', 'subject'], axis = 1).values\n",
    "trainLabel = train.Activity.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding labels: create encoder, fit data into encoder, and then transform it\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encoding test labels\n",
    "encoder.fit(testLabel)\n",
    "testLabelE = encoder.transform(testLabel)\n",
    "\n",
    "#encoding train labels\n",
    "encoder.fit(trainLabel)\n",
    "trainLabelE = encoder.transform(trainLabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models:\n",
    "\n",
    "1. Neural Network \n",
    "2. Logistic Regression\n",
    "\n",
    "Decision Tree\n",
    "2. SVM\n",
    "3. NN\n",
    "4. RF\n",
    "5. GBM\n",
    "6. DNN\n",
    "7. ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying supervised neural network using multi layer perception\n",
    "\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "mlpsgd = nn.MLPClassifier(hidden_layer_sizes=(90,) \\\n",
    "                         , max_iter=2000, alpha=1e-4 \\\n",
    "                         , solver='sgd', verbose=10 \\\n",
    "                         , tol=1e-19, random_state=1 \\\n",
    "                         , learning_rate_init=.001)\n",
    "\n",
    "mlpadam = nn.MLPClassifier(hidden_layer_sizes=(90,) \\\n",
    "                         , max_iter=1000, alpha=1e-4 \\\n",
    "                         , solver='adam', verbose=10 \\\n",
    "                         , tol=1e-19, random_state=1 \\\n",
    "                         , learning_rate_init=.001)\n",
    "\n",
    "mlplbfgs = nn.MLPClassifier(hidden_layer_sizes=(90,) \\\n",
    "                         , max_iter=1000, alpha=1e-4 \\\n",
    "                         , solver='lbfgs', verbose=10 \\\n",
    "                         , tol=1e-19, random_state=1 \\\n",
    "                         , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91456151\n",
      "Iteration 2, loss = 1.64972319\n",
      "Iteration 3, loss = 1.50984699\n",
      "Iteration 4, loss = 1.39466663\n",
      "Iteration 5, loss = 1.28814416\n",
      "Iteration 6, loss = 1.20469260\n",
      "Iteration 7, loss = 1.13516823\n",
      "Iteration 8, loss = 1.07502719\n",
      "Iteration 9, loss = 1.02757929\n",
      "Iteration 10, loss = 0.98263413\n",
      "Iteration 11, loss = 0.94495166\n",
      "Iteration 12, loss = 0.91127120\n",
      "Iteration 13, loss = 0.88070513\n",
      "Iteration 14, loss = 0.85161086\n",
      "Iteration 15, loss = 0.82546890\n",
      "Iteration 16, loss = 0.80074584\n",
      "Iteration 17, loss = 0.77769929\n",
      "Iteration 18, loss = 0.75728129\n",
      "Iteration 19, loss = 0.73715073\n",
      "Iteration 20, loss = 0.71807709\n",
      "Iteration 21, loss = 0.70042105\n",
      "Iteration 22, loss = 0.68379387\n",
      "Iteration 23, loss = 0.66784085\n",
      "Iteration 24, loss = 0.65268941\n",
      "Iteration 25, loss = 0.63788353\n",
      "Iteration 26, loss = 0.62408540\n",
      "Iteration 27, loss = 0.61050342\n",
      "Iteration 28, loss = 0.59803338\n",
      "Iteration 29, loss = 0.58598380\n",
      "Iteration 30, loss = 0.57365622\n",
      "Iteration 31, loss = 0.56333646\n",
      "Iteration 32, loss = 0.55138514\n",
      "Iteration 33, loss = 0.54106732\n",
      "Iteration 34, loss = 0.53092025\n",
      "Iteration 35, loss = 0.52205896\n",
      "Iteration 36, loss = 0.51173484\n",
      "Iteration 37, loss = 0.50287123\n",
      "Iteration 38, loss = 0.49453705\n",
      "Iteration 39, loss = 0.48528599\n",
      "Iteration 40, loss = 0.47733865\n",
      "Iteration 41, loss = 0.46973373\n",
      "Iteration 42, loss = 0.46143132\n",
      "Iteration 43, loss = 0.45426971\n",
      "Iteration 44, loss = 0.44651365\n",
      "Iteration 45, loss = 0.43964172\n",
      "Iteration 46, loss = 0.43274257\n",
      "Iteration 47, loss = 0.42662422\n",
      "Iteration 48, loss = 0.41955791\n",
      "Iteration 49, loss = 0.41391919\n",
      "Iteration 50, loss = 0.40782895\n",
      "Iteration 51, loss = 0.40171146\n",
      "Iteration 52, loss = 0.39584231\n",
      "Iteration 53, loss = 0.39054119\n",
      "Iteration 54, loss = 0.38496910\n",
      "Iteration 55, loss = 0.37946282\n",
      "Iteration 56, loss = 0.37447216\n",
      "Iteration 57, loss = 0.36991257\n",
      "Iteration 58, loss = 0.36448979\n",
      "Iteration 59, loss = 0.35955792\n",
      "Iteration 60, loss = 0.35509312\n",
      "Iteration 61, loss = 0.35107778\n",
      "Iteration 62, loss = 0.34590222\n",
      "Iteration 63, loss = 0.34185601\n",
      "Iteration 64, loss = 0.33760725\n",
      "Iteration 65, loss = 0.33312739\n",
      "Iteration 66, loss = 0.32921826\n",
      "Iteration 67, loss = 0.32495925\n",
      "Iteration 68, loss = 0.32098263\n",
      "Iteration 69, loss = 0.31716377\n",
      "Iteration 70, loss = 0.31365312\n",
      "Iteration 71, loss = 0.30966083\n",
      "Iteration 72, loss = 0.30621319\n",
      "Iteration 73, loss = 0.30275735\n",
      "Iteration 74, loss = 0.29948299\n",
      "Iteration 75, loss = 0.29608087\n",
      "Iteration 76, loss = 0.29308528\n",
      "Iteration 77, loss = 0.28962988\n",
      "Iteration 78, loss = 0.28671812\n",
      "Iteration 79, loss = 0.28333519\n",
      "Iteration 80, loss = 0.28072683\n",
      "Iteration 81, loss = 0.27755836\n",
      "Iteration 82, loss = 0.27453667\n",
      "Iteration 83, loss = 0.27170159\n",
      "Iteration 84, loss = 0.26911811\n",
      "Iteration 85, loss = 0.26653146\n",
      "Iteration 86, loss = 0.26379604\n",
      "Iteration 87, loss = 0.26138659\n",
      "Iteration 88, loss = 0.25869873\n",
      "Iteration 89, loss = 0.25606999\n",
      "Iteration 90, loss = 0.25397579\n",
      "Iteration 91, loss = 0.25136578\n",
      "Iteration 92, loss = 0.24897927\n",
      "Iteration 93, loss = 0.24660794\n",
      "Iteration 94, loss = 0.24431870\n",
      "Iteration 95, loss = 0.24263456\n",
      "Iteration 96, loss = 0.24027554\n",
      "Iteration 97, loss = 0.23801807\n",
      "Iteration 98, loss = 0.23574774\n",
      "Iteration 99, loss = 0.23361207\n",
      "Iteration 100, loss = 0.23197076\n",
      "Iteration 101, loss = 0.22976462\n",
      "Iteration 102, loss = 0.22780719\n",
      "Iteration 103, loss = 0.22589126\n",
      "Iteration 104, loss = 0.22362862\n",
      "Iteration 105, loss = 0.22173560\n",
      "Iteration 106, loss = 0.22018499\n",
      "Iteration 107, loss = 0.21821020\n",
      "Iteration 108, loss = 0.21641303\n",
      "Iteration 109, loss = 0.21487272\n",
      "Iteration 110, loss = 0.21322895\n",
      "Iteration 111, loss = 0.21147415\n",
      "Iteration 112, loss = 0.20954328\n",
      "Iteration 113, loss = 0.20825630\n",
      "Iteration 114, loss = 0.20618664\n",
      "Iteration 115, loss = 0.20466914\n",
      "Iteration 116, loss = 0.20312007\n",
      "Iteration 117, loss = 0.20168418\n",
      "Iteration 118, loss = 0.19996635\n",
      "Iteration 119, loss = 0.19869664\n",
      "Iteration 120, loss = 0.19702024\n",
      "Iteration 121, loss = 0.19582642\n",
      "Iteration 122, loss = 0.19411027\n",
      "Iteration 123, loss = 0.19268202\n",
      "Iteration 124, loss = 0.19127476\n",
      "Iteration 125, loss = 0.19000114\n",
      "Iteration 126, loss = 0.18862121\n",
      "Iteration 127, loss = 0.18701540\n",
      "Iteration 128, loss = 0.18579504\n",
      "Iteration 129, loss = 0.18490884\n",
      "Iteration 130, loss = 0.18332523\n",
      "Iteration 131, loss = 0.18202154\n",
      "Iteration 132, loss = 0.18106391\n",
      "Iteration 133, loss = 0.17935510\n",
      "Iteration 134, loss = 0.17855029\n",
      "Iteration 135, loss = 0.17694660\n",
      "Iteration 136, loss = 0.17582443\n",
      "Iteration 137, loss = 0.17464267\n",
      "Iteration 138, loss = 0.17345502\n",
      "Iteration 139, loss = 0.17213571\n",
      "Iteration 140, loss = 0.17121074\n",
      "Iteration 141, loss = 0.16994190\n",
      "Iteration 142, loss = 0.16886714\n",
      "Iteration 143, loss = 0.16803656\n",
      "Iteration 144, loss = 0.16715282\n",
      "Iteration 145, loss = 0.16635637\n",
      "Iteration 146, loss = 0.16456465\n",
      "Iteration 147, loss = 0.16363876\n",
      "Iteration 148, loss = 0.16250360\n",
      "Iteration 149, loss = 0.16166965\n",
      "Iteration 150, loss = 0.16073906\n",
      "Iteration 151, loss = 0.15949640\n",
      "Iteration 152, loss = 0.15856380\n",
      "Iteration 153, loss = 0.15770850\n",
      "Iteration 154, loss = 0.15686324\n",
      "Iteration 155, loss = 0.15565451\n",
      "Iteration 156, loss = 0.15477746\n",
      "Iteration 157, loss = 0.15394249\n",
      "Iteration 158, loss = 0.15292131\n",
      "Iteration 159, loss = 0.15187252\n",
      "Iteration 160, loss = 0.15118726\n",
      "Iteration 161, loss = 0.15027073\n",
      "Iteration 162, loss = 0.14930930\n",
      "Iteration 163, loss = 0.14872604\n",
      "Iteration 164, loss = 0.14774833\n",
      "Iteration 165, loss = 0.14692727\n",
      "Iteration 166, loss = 0.14591053\n",
      "Iteration 167, loss = 0.14512885\n",
      "Iteration 168, loss = 0.14431183\n",
      "Iteration 169, loss = 0.14342957\n",
      "Iteration 170, loss = 0.14250930\n",
      "Iteration 171, loss = 0.14182595\n",
      "Iteration 172, loss = 0.14102726\n",
      "Iteration 173, loss = 0.14039218\n",
      "Iteration 174, loss = 0.13937578\n",
      "Iteration 175, loss = 0.13869327\n",
      "Iteration 176, loss = 0.13797049\n",
      "Iteration 177, loss = 0.13765768\n",
      "Iteration 178, loss = 0.13644216\n",
      "Iteration 179, loss = 0.13574607\n",
      "Iteration 180, loss = 0.13503711\n",
      "Iteration 181, loss = 0.13432834\n",
      "Iteration 182, loss = 0.13397250\n",
      "Iteration 183, loss = 0.13298348\n",
      "Iteration 184, loss = 0.13223196\n",
      "Iteration 185, loss = 0.13145456\n",
      "Iteration 186, loss = 0.13082482\n",
      "Iteration 187, loss = 0.13007863\n",
      "Iteration 188, loss = 0.12947145\n",
      "Iteration 189, loss = 0.12883584\n",
      "Iteration 190, loss = 0.12814274\n",
      "Iteration 191, loss = 0.12745010\n",
      "Iteration 192, loss = 0.12680814\n",
      "Iteration 193, loss = 0.12666855\n",
      "Iteration 194, loss = 0.12588818\n",
      "Iteration 195, loss = 0.12505701\n",
      "Iteration 196, loss = 0.12431395\n",
      "Iteration 197, loss = 0.12381857\n",
      "Iteration 198, loss = 0.12314025\n",
      "Iteration 199, loss = 0.12269326\n",
      "Iteration 200, loss = 0.12183703\n",
      "Iteration 201, loss = 0.12133270\n",
      "Iteration 202, loss = 0.12083467\n",
      "Iteration 203, loss = 0.12012713\n",
      "Iteration 204, loss = 0.11960358\n",
      "Iteration 205, loss = 0.11914732\n",
      "Iteration 206, loss = 0.11846115\n",
      "Iteration 207, loss = 0.11780837\n",
      "Iteration 208, loss = 0.11733110\n",
      "Iteration 209, loss = 0.11691333\n",
      "Iteration 210, loss = 0.11622447\n",
      "Iteration 211, loss = 0.11567243\n",
      "Iteration 212, loss = 0.11519417\n",
      "Iteration 213, loss = 0.11457989\n",
      "Iteration 214, loss = 0.11403517\n",
      "Iteration 215, loss = 0.11354853\n",
      "Iteration 216, loss = 0.11299918\n",
      "Iteration 217, loss = 0.11259380\n",
      "Iteration 218, loss = 0.11190091\n",
      "Iteration 219, loss = 0.11155843\n",
      "Iteration 220, loss = 0.11126204\n",
      "Iteration 221, loss = 0.11046001\n",
      "Iteration 222, loss = 0.11010877\n",
      "Iteration 223, loss = 0.10947792\n",
      "Iteration 224, loss = 0.10896442\n",
      "Iteration 225, loss = 0.10869450\n",
      "Iteration 226, loss = 0.10847283\n",
      "Iteration 227, loss = 0.10745402\n",
      "Iteration 228, loss = 0.10724991\n",
      "Iteration 229, loss = 0.10678833\n",
      "Iteration 230, loss = 0.10615458\n",
      "Iteration 231, loss = 0.10564441\n",
      "Iteration 232, loss = 0.10520433\n",
      "Iteration 233, loss = 0.10486395\n",
      "Iteration 234, loss = 0.10440514\n",
      "Iteration 235, loss = 0.10431662\n",
      "Iteration 236, loss = 0.10339420\n",
      "Iteration 237, loss = 0.10323669\n",
      "Iteration 238, loss = 0.10267130\n",
      "Iteration 239, loss = 0.10213626\n",
      "Iteration 240, loss = 0.10195482\n",
      "Iteration 241, loss = 0.10138008\n",
      "Iteration 242, loss = 0.10087288\n",
      "Iteration 243, loss = 0.10044817\n",
      "Iteration 244, loss = 0.10003140\n",
      "Iteration 245, loss = 0.09959980\n",
      "Iteration 246, loss = 0.09918859\n",
      "Iteration 247, loss = 0.09885166\n",
      "Iteration 248, loss = 0.09843303\n",
      "Iteration 249, loss = 0.09798591\n",
      "Iteration 250, loss = 0.09769093\n",
      "Iteration 251, loss = 0.09717729\n",
      "Iteration 252, loss = 0.09686982\n",
      "Iteration 253, loss = 0.09645067\n",
      "Iteration 254, loss = 0.09599532\n",
      "Iteration 255, loss = 0.09559032\n",
      "Iteration 256, loss = 0.09533774\n",
      "Iteration 257, loss = 0.09491504\n",
      "Iteration 258, loss = 0.09474769\n",
      "Iteration 259, loss = 0.09426872\n",
      "Iteration 260, loss = 0.09389148\n",
      "Iteration 261, loss = 0.09356011\n",
      "Iteration 262, loss = 0.09314704\n",
      "Iteration 263, loss = 0.09262378\n",
      "Iteration 264, loss = 0.09228753\n",
      "Iteration 265, loss = 0.09199609\n",
      "Iteration 266, loss = 0.09163994\n",
      "Iteration 267, loss = 0.09130785\n",
      "Iteration 268, loss = 0.09113926\n",
      "Iteration 269, loss = 0.09053250\n",
      "Iteration 270, loss = 0.09026489\n",
      "Iteration 271, loss = 0.08989283\n",
      "Iteration 272, loss = 0.08959909\n",
      "Iteration 273, loss = 0.09000444\n",
      "Iteration 274, loss = 0.08890823\n",
      "Iteration 275, loss = 0.08865057\n",
      "Iteration 276, loss = 0.08813165\n",
      "Iteration 277, loss = 0.08794744\n",
      "Iteration 278, loss = 0.08756771\n",
      "Iteration 279, loss = 0.08721481\n",
      "Iteration 280, loss = 0.08695256\n",
      "Iteration 281, loss = 0.08661328\n",
      "Iteration 282, loss = 0.08622158\n",
      "Iteration 283, loss = 0.08605692\n",
      "Iteration 284, loss = 0.08553515\n",
      "Iteration 285, loss = 0.08560781\n",
      "Iteration 286, loss = 0.08526182\n",
      "Iteration 287, loss = 0.08465508\n",
      "Iteration 288, loss = 0.08430624\n",
      "Iteration 289, loss = 0.08407374\n",
      "Iteration 290, loss = 0.08372813\n",
      "Iteration 291, loss = 0.08361309\n",
      "Iteration 292, loss = 0.08314337\n",
      "Iteration 293, loss = 0.08291487\n",
      "Iteration 294, loss = 0.08272012\n",
      "Iteration 295, loss = 0.08223814\n",
      "Iteration 296, loss = 0.08200990\n",
      "Iteration 297, loss = 0.08165804\n",
      "Iteration 298, loss = 0.08168614\n",
      "Iteration 299, loss = 0.08117337\n",
      "Iteration 300, loss = 0.08090874\n",
      "Iteration 301, loss = 0.08072476\n",
      "Iteration 302, loss = 0.08025634\n",
      "Iteration 303, loss = 0.08037087\n",
      "Iteration 304, loss = 0.07988500\n",
      "Iteration 305, loss = 0.07947441\n",
      "Iteration 306, loss = 0.07918980\n",
      "Iteration 307, loss = 0.07899056\n",
      "Iteration 308, loss = 0.07854547\n",
      "Iteration 309, loss = 0.07835731\n",
      "Iteration 310, loss = 0.07811949\n",
      "Iteration 311, loss = 0.07786510\n",
      "Iteration 312, loss = 0.07755149\n",
      "Iteration 313, loss = 0.07726272\n",
      "Iteration 314, loss = 0.07707836\n",
      "Iteration 315, loss = 0.07685609\n",
      "Iteration 316, loss = 0.07649476\n",
      "Iteration 317, loss = 0.07628135\n",
      "Iteration 318, loss = 0.07593707\n",
      "Iteration 319, loss = 0.07576232\n",
      "Iteration 320, loss = 0.07563293\n",
      "Iteration 321, loss = 0.07529117\n",
      "Iteration 322, loss = 0.07505031\n",
      "Iteration 323, loss = 0.07473184\n",
      "Iteration 324, loss = 0.07451024\n",
      "Iteration 325, loss = 0.07422806\n",
      "Iteration 326, loss = 0.07396834\n",
      "Iteration 327, loss = 0.07373253\n",
      "Iteration 328, loss = 0.07350125\n",
      "Iteration 329, loss = 0.07330779\n",
      "Iteration 330, loss = 0.07303045\n",
      "Iteration 331, loss = 0.07296840\n",
      "Iteration 332, loss = 0.07275139\n",
      "Iteration 333, loss = 0.07229811\n",
      "Iteration 334, loss = 0.07240026\n",
      "Iteration 335, loss = 0.07191797\n",
      "Iteration 336, loss = 0.07171834\n",
      "Iteration 337, loss = 0.07156730\n",
      "Iteration 338, loss = 0.07124094\n",
      "Iteration 339, loss = 0.07103329\n",
      "Iteration 340, loss = 0.07090090\n",
      "Iteration 341, loss = 0.07065192\n",
      "Iteration 342, loss = 0.07024840\n",
      "Iteration 343, loss = 0.07001296\n",
      "Iteration 344, loss = 0.06995050\n",
      "Iteration 345, loss = 0.06956769\n",
      "Iteration 346, loss = 0.06949430\n",
      "Iteration 347, loss = 0.06928359\n",
      "Iteration 348, loss = 0.06932269\n",
      "Iteration 349, loss = 0.06885040\n",
      "Iteration 350, loss = 0.06869324\n",
      "Iteration 351, loss = 0.06831812\n",
      "Iteration 352, loss = 0.06809123\n",
      "Iteration 353, loss = 0.06787463\n",
      "Iteration 354, loss = 0.06771759\n",
      "Iteration 355, loss = 0.06759514\n",
      "Iteration 356, loss = 0.06733858\n",
      "Iteration 357, loss = 0.06705677\n",
      "Iteration 358, loss = 0.06690186\n",
      "Iteration 359, loss = 0.06675273\n",
      "Iteration 360, loss = 0.06645494\n",
      "Iteration 361, loss = 0.06647906\n",
      "Iteration 362, loss = 0.06640616\n",
      "Iteration 363, loss = 0.06612231\n",
      "Iteration 364, loss = 0.06570451\n",
      "Iteration 365, loss = 0.06543526\n",
      "Iteration 366, loss = 0.06527756\n",
      "Iteration 367, loss = 0.06504952\n",
      "Iteration 368, loss = 0.06501905\n",
      "Iteration 369, loss = 0.06471815\n",
      "Iteration 370, loss = 0.06458039\n",
      "Iteration 371, loss = 0.06465250\n",
      "Iteration 372, loss = 0.06474786\n",
      "Iteration 373, loss = 0.06387944\n",
      "Iteration 374, loss = 0.06379054\n",
      "Iteration 375, loss = 0.06365400\n",
      "Iteration 376, loss = 0.06363623\n",
      "Iteration 377, loss = 0.06318422\n",
      "Iteration 378, loss = 0.06314981\n",
      "Iteration 379, loss = 0.06279584\n",
      "Iteration 380, loss = 0.06262660\n",
      "Iteration 381, loss = 0.06241727\n",
      "Iteration 382, loss = 0.06231435\n",
      "Iteration 383, loss = 0.06215140\n",
      "Iteration 384, loss = 0.06189590\n",
      "Iteration 385, loss = 0.06185662\n",
      "Iteration 386, loss = 0.06155048\n",
      "Iteration 387, loss = 0.06138377\n",
      "Iteration 388, loss = 0.06116891\n",
      "Iteration 389, loss = 0.06103306\n",
      "Iteration 390, loss = 0.06087986\n",
      "Iteration 391, loss = 0.06070030\n",
      "Iteration 392, loss = 0.06053730\n",
      "Iteration 393, loss = 0.06043618\n",
      "Iteration 394, loss = 0.06014786\n",
      "Iteration 395, loss = 0.05999542\n",
      "Iteration 396, loss = 0.05984973\n",
      "Iteration 397, loss = 0.05976578\n",
      "Iteration 398, loss = 0.05950388\n",
      "Iteration 399, loss = 0.05933924\n",
      "Iteration 400, loss = 0.05933293\n",
      "Iteration 401, loss = 0.05896338\n",
      "Iteration 402, loss = 0.05886597\n",
      "Iteration 403, loss = 0.05863596\n",
      "Iteration 404, loss = 0.05871244\n",
      "Iteration 405, loss = 0.05839648\n",
      "Iteration 406, loss = 0.05835372\n",
      "Iteration 407, loss = 0.05817868\n",
      "Iteration 408, loss = 0.05808695\n",
      "Iteration 409, loss = 0.05794414\n",
      "Iteration 410, loss = 0.05770889\n",
      "Iteration 411, loss = 0.05755138\n",
      "Iteration 412, loss = 0.05735013\n",
      "Iteration 413, loss = 0.05719157\n",
      "Iteration 414, loss = 0.05701574\n",
      "Iteration 415, loss = 0.05693684\n",
      "Iteration 416, loss = 0.05683337\n",
      "Iteration 417, loss = 0.05646313\n",
      "Iteration 418, loss = 0.05630685\n",
      "Iteration 419, loss = 0.05632117\n",
      "Iteration 420, loss = 0.05604847\n",
      "Iteration 421, loss = 0.05609732\n",
      "Iteration 422, loss = 0.05570278\n",
      "Iteration 423, loss = 0.05558430\n",
      "Iteration 424, loss = 0.05546780\n",
      "Iteration 425, loss = 0.05530906\n",
      "Iteration 426, loss = 0.05523411\n",
      "Iteration 427, loss = 0.05496536\n",
      "Iteration 428, loss = 0.05484101\n",
      "Iteration 429, loss = 0.05472127\n",
      "Iteration 430, loss = 0.05462801\n",
      "Iteration 431, loss = 0.05452086\n",
      "Iteration 432, loss = 0.05438975\n",
      "Iteration 433, loss = 0.05419760\n",
      "Iteration 434, loss = 0.05417285\n",
      "Iteration 435, loss = 0.05403749\n",
      "Iteration 436, loss = 0.05382678\n",
      "Iteration 437, loss = 0.05357938\n",
      "Iteration 438, loss = 0.05339973\n",
      "Iteration 439, loss = 0.05339092\n",
      "Iteration 440, loss = 0.05341674\n",
      "Iteration 441, loss = 0.05314262\n",
      "Iteration 442, loss = 0.05301554\n",
      "Iteration 443, loss = 0.05276000\n",
      "Iteration 444, loss = 0.05293196\n",
      "Iteration 445, loss = 0.05243808\n",
      "Iteration 446, loss = 0.05240706\n",
      "Iteration 447, loss = 0.05222881\n",
      "Iteration 448, loss = 0.05207678\n",
      "Iteration 449, loss = 0.05198441\n",
      "Iteration 450, loss = 0.05182920\n",
      "Iteration 451, loss = 0.05169658\n",
      "Iteration 452, loss = 0.05154729\n",
      "Iteration 453, loss = 0.05155981\n",
      "Iteration 454, loss = 0.05127060\n",
      "Iteration 455, loss = 0.05130531\n",
      "Iteration 456, loss = 0.05114013\n",
      "Iteration 457, loss = 0.05094058\n",
      "Iteration 458, loss = 0.05079220\n",
      "Iteration 459, loss = 0.05069730\n",
      "Iteration 460, loss = 0.05063597\n",
      "Iteration 461, loss = 0.05046013\n",
      "Iteration 462, loss = 0.05048011\n",
      "Iteration 463, loss = 0.05014849\n",
      "Iteration 464, loss = 0.05010599\n",
      "Iteration 465, loss = 0.04993835\n",
      "Iteration 466, loss = 0.04977271\n",
      "Iteration 467, loss = 0.04989206\n",
      "Iteration 468, loss = 0.04962716\n",
      "Iteration 469, loss = 0.04951227\n",
      "Iteration 470, loss = 0.04934347\n",
      "Iteration 471, loss = 0.04933273\n",
      "Iteration 472, loss = 0.04907982\n",
      "Iteration 473, loss = 0.04903616\n",
      "Iteration 474, loss = 0.04894639\n",
      "Iteration 475, loss = 0.04892128\n",
      "Iteration 476, loss = 0.04856239\n",
      "Iteration 477, loss = 0.04847212\n",
      "Iteration 478, loss = 0.04856038\n",
      "Iteration 479, loss = 0.04838237\n",
      "Iteration 480, loss = 0.04820802\n",
      "Iteration 481, loss = 0.04800333\n",
      "Iteration 482, loss = 0.04787950\n",
      "Iteration 483, loss = 0.04786165\n",
      "Iteration 484, loss = 0.04778309\n",
      "Iteration 485, loss = 0.04757229\n",
      "Iteration 486, loss = 0.04759306\n",
      "Iteration 487, loss = 0.04741719\n",
      "Iteration 488, loss = 0.04746371\n",
      "Iteration 489, loss = 0.04719157\n",
      "Iteration 490, loss = 0.04702019\n",
      "Iteration 491, loss = 0.04704216\n",
      "Iteration 492, loss = 0.04677462\n",
      "Iteration 493, loss = 0.04691717\n",
      "Iteration 494, loss = 0.04658257\n",
      "Iteration 495, loss = 0.04643874\n",
      "Iteration 496, loss = 0.04632006\n",
      "Iteration 497, loss = 0.04623268\n",
      "Iteration 498, loss = 0.04617833\n",
      "Iteration 499, loss = 0.04608892\n",
      "Iteration 500, loss = 0.04589783\n",
      "Iteration 501, loss = 0.04587286\n",
      "Iteration 502, loss = 0.04579837\n",
      "Iteration 503, loss = 0.04566196\n",
      "Iteration 504, loss = 0.04553295\n",
      "Iteration 505, loss = 0.04541610\n",
      "Iteration 506, loss = 0.04530071\n",
      "Iteration 507, loss = 0.04515991\n",
      "Iteration 508, loss = 0.04517430\n",
      "Iteration 509, loss = 0.04500758\n",
      "Iteration 510, loss = 0.04489650\n",
      "Iteration 511, loss = 0.04475363\n",
      "Iteration 512, loss = 0.04464928\n",
      "Iteration 513, loss = 0.04454084\n",
      "Iteration 514, loss = 0.04455512\n",
      "Iteration 515, loss = 0.04439811\n",
      "Iteration 516, loss = 0.04429562\n",
      "Iteration 517, loss = 0.04425832\n",
      "Iteration 518, loss = 0.04409139\n",
      "Iteration 519, loss = 0.04404737\n",
      "Iteration 520, loss = 0.04406237\n",
      "Iteration 521, loss = 0.04371264\n",
      "Iteration 522, loss = 0.04370390\n",
      "Iteration 523, loss = 0.04360465\n",
      "Iteration 524, loss = 0.04350340\n",
      "Iteration 525, loss = 0.04344911\n",
      "Iteration 526, loss = 0.04330778\n",
      "Iteration 527, loss = 0.04318688\n",
      "Iteration 528, loss = 0.04320454\n",
      "Iteration 529, loss = 0.04303523\n",
      "Iteration 530, loss = 0.04298097\n",
      "Iteration 531, loss = 0.04278683\n",
      "Iteration 532, loss = 0.04287351\n",
      "Iteration 533, loss = 0.04258851\n",
      "Iteration 534, loss = 0.04253802\n",
      "Iteration 535, loss = 0.04248498\n",
      "Iteration 536, loss = 0.04232437\n",
      "Iteration 537, loss = 0.04235076\n",
      "Iteration 538, loss = 0.04244550\n",
      "Iteration 539, loss = 0.04201594\n",
      "Iteration 540, loss = 0.04196478\n",
      "Iteration 541, loss = 0.04192820\n",
      "Iteration 542, loss = 0.04179287\n",
      "Iteration 543, loss = 0.04172459\n",
      "Iteration 544, loss = 0.04165107\n",
      "Iteration 545, loss = 0.04165470\n",
      "Iteration 546, loss = 0.04145703\n",
      "Iteration 547, loss = 0.04142594\n",
      "Iteration 548, loss = 0.04125090\n",
      "Iteration 549, loss = 0.04120571\n",
      "Iteration 550, loss = 0.04107916\n",
      "Iteration 551, loss = 0.04120840\n",
      "Iteration 552, loss = 0.04099251\n",
      "Iteration 553, loss = 0.04090522\n",
      "Iteration 554, loss = 0.04091025\n",
      "Iteration 555, loss = 0.04068711\n",
      "Iteration 556, loss = 0.04056754\n",
      "Iteration 557, loss = 0.04050904\n",
      "Iteration 558, loss = 0.04040371\n",
      "Iteration 559, loss = 0.04037384\n",
      "Iteration 560, loss = 0.04026647\n",
      "Iteration 561, loss = 0.04014068\n",
      "Iteration 562, loss = 0.04007198\n",
      "Iteration 563, loss = 0.03995202\n",
      "Iteration 564, loss = 0.03989807\n",
      "Iteration 565, loss = 0.03981830\n",
      "Iteration 566, loss = 0.03980171\n",
      "Iteration 567, loss = 0.03968929\n",
      "Iteration 568, loss = 0.03987767\n",
      "Iteration 569, loss = 0.03954079\n",
      "Iteration 570, loss = 0.03945559\n",
      "Iteration 571, loss = 0.03932468\n",
      "Iteration 572, loss = 0.03929350\n",
      "Iteration 573, loss = 0.03930549\n",
      "Iteration 574, loss = 0.03909148\n",
      "Iteration 575, loss = 0.03905128\n",
      "Iteration 576, loss = 0.03897013\n",
      "Iteration 577, loss = 0.03888156\n",
      "Iteration 578, loss = 0.03878415\n",
      "Iteration 579, loss = 0.03870718\n",
      "Iteration 580, loss = 0.03866002\n",
      "Iteration 581, loss = 0.03847287\n",
      "Iteration 582, loss = 0.03841957\n",
      "Iteration 583, loss = 0.03834233\n",
      "Iteration 584, loss = 0.03825959\n",
      "Iteration 585, loss = 0.03818444\n",
      "Iteration 586, loss = 0.03814085\n",
      "Iteration 587, loss = 0.03804839\n",
      "Iteration 588, loss = 0.03798658\n",
      "Iteration 589, loss = 0.03798682\n",
      "Iteration 590, loss = 0.03783530\n",
      "Iteration 591, loss = 0.03792317\n",
      "Iteration 592, loss = 0.03765989\n",
      "Iteration 593, loss = 0.03755379\n",
      "Iteration 594, loss = 0.03750554\n",
      "Iteration 595, loss = 0.03745662\n",
      "Iteration 596, loss = 0.03735559\n",
      "Iteration 597, loss = 0.03727409\n",
      "Iteration 598, loss = 0.03730948\n",
      "Iteration 599, loss = 0.03719672\n",
      "Iteration 600, loss = 0.03711260\n",
      "Iteration 601, loss = 0.03723868\n",
      "Iteration 602, loss = 0.03694476\n",
      "Iteration 603, loss = 0.03693066\n",
      "Iteration 604, loss = 0.03685669\n",
      "Iteration 605, loss = 0.03667522\n",
      "Iteration 606, loss = 0.03663791\n",
      "Iteration 607, loss = 0.03654055\n",
      "Iteration 608, loss = 0.03651301\n",
      "Iteration 609, loss = 0.03642897\n",
      "Iteration 610, loss = 0.03641934\n",
      "Iteration 611, loss = 0.03624410\n",
      "Iteration 612, loss = 0.03630210\n",
      "Iteration 613, loss = 0.03619225\n",
      "Iteration 614, loss = 0.03626244\n",
      "Iteration 615, loss = 0.03600056\n",
      "Iteration 616, loss = 0.03593126\n",
      "Iteration 617, loss = 0.03597523\n",
      "Iteration 618, loss = 0.03572811\n",
      "Iteration 619, loss = 0.03582573\n",
      "Iteration 620, loss = 0.03567963\n",
      "Iteration 621, loss = 0.03572050\n",
      "Iteration 622, loss = 0.03560478\n",
      "Iteration 623, loss = 0.03549646\n",
      "Iteration 624, loss = 0.03554068\n",
      "Iteration 625, loss = 0.03527738\n",
      "Iteration 626, loss = 0.03532647\n",
      "Iteration 627, loss = 0.03515301\n",
      "Iteration 628, loss = 0.03509391\n",
      "Iteration 629, loss = 0.03513259\n",
      "Iteration 630, loss = 0.03493600\n",
      "Iteration 631, loss = 0.03495005\n",
      "Iteration 632, loss = 0.03489767\n",
      "Iteration 633, loss = 0.03476432\n",
      "Iteration 634, loss = 0.03472505\n",
      "Iteration 635, loss = 0.03470251\n",
      "Iteration 636, loss = 0.03454283\n",
      "Iteration 637, loss = 0.03450246\n",
      "Iteration 638, loss = 0.03446307\n",
      "Iteration 639, loss = 0.03447439\n",
      "Iteration 640, loss = 0.03430239\n",
      "Iteration 641, loss = 0.03428430\n",
      "Iteration 642, loss = 0.03420691\n",
      "Iteration 643, loss = 0.03409661\n",
      "Iteration 644, loss = 0.03415641\n",
      "Iteration 645, loss = 0.03398479\n",
      "Iteration 646, loss = 0.03396197\n",
      "Iteration 647, loss = 0.03397541\n",
      "Iteration 648, loss = 0.03396018\n",
      "Iteration 649, loss = 0.03378746\n",
      "Iteration 650, loss = 0.03369163\n",
      "Iteration 651, loss = 0.03363735\n",
      "Iteration 652, loss = 0.03362184\n",
      "Iteration 653, loss = 0.03344742\n",
      "Iteration 654, loss = 0.03339358\n",
      "Iteration 655, loss = 0.03343639\n",
      "Iteration 656, loss = 0.03336613\n",
      "Iteration 657, loss = 0.03324208\n",
      "Iteration 658, loss = 0.03315791\n",
      "Iteration 659, loss = 0.03316443\n",
      "Iteration 660, loss = 0.03312230\n",
      "Iteration 661, loss = 0.03301955\n",
      "Iteration 662, loss = 0.03291536\n",
      "Iteration 663, loss = 0.03285843\n",
      "Iteration 664, loss = 0.03286958\n",
      "Iteration 665, loss = 0.03276013\n",
      "Iteration 666, loss = 0.03273759\n",
      "Iteration 667, loss = 0.03260668\n",
      "Iteration 668, loss = 0.03256798\n",
      "Iteration 669, loss = 0.03252405\n",
      "Iteration 670, loss = 0.03245423\n",
      "Iteration 671, loss = 0.03241373\n",
      "Iteration 672, loss = 0.03239516\n",
      "Iteration 673, loss = 0.03232336\n",
      "Iteration 674, loss = 0.03232126\n",
      "Iteration 675, loss = 0.03221520\n",
      "Iteration 676, loss = 0.03208808\n",
      "Iteration 677, loss = 0.03208766\n",
      "Iteration 678, loss = 0.03201856\n",
      "Iteration 679, loss = 0.03195121\n",
      "Iteration 680, loss = 0.03190517\n",
      "Iteration 681, loss = 0.03179605\n",
      "Iteration 682, loss = 0.03183292\n",
      "Iteration 683, loss = 0.03180547\n",
      "Iteration 684, loss = 0.03164103\n",
      "Iteration 685, loss = 0.03165736\n",
      "Iteration 686, loss = 0.03151244\n",
      "Iteration 687, loss = 0.03154714\n",
      "Iteration 688, loss = 0.03146062\n",
      "Iteration 689, loss = 0.03141389\n",
      "Iteration 690, loss = 0.03138558\n",
      "Iteration 691, loss = 0.03125580\n",
      "Iteration 692, loss = 0.03122985\n",
      "Iteration 693, loss = 0.03114129\n",
      "Iteration 694, loss = 0.03114392\n",
      "Iteration 695, loss = 0.03117127\n",
      "Iteration 696, loss = 0.03103808\n",
      "Iteration 697, loss = 0.03113712\n",
      "Iteration 698, loss = 0.03089457\n",
      "Iteration 699, loss = 0.03091958\n",
      "Iteration 700, loss = 0.03080623\n",
      "Iteration 701, loss = 0.03073934\n",
      "Iteration 702, loss = 0.03072134\n",
      "Iteration 703, loss = 0.03066382\n",
      "Iteration 704, loss = 0.03056209\n",
      "Iteration 705, loss = 0.03050707\n",
      "Iteration 706, loss = 0.03044184\n",
      "Iteration 707, loss = 0.03044826\n",
      "Iteration 708, loss = 0.03032720\n",
      "Iteration 709, loss = 0.03030094\n",
      "Iteration 710, loss = 0.03025422\n",
      "Iteration 711, loss = 0.03020628\n",
      "Iteration 712, loss = 0.03012324\n",
      "Iteration 713, loss = 0.03011072\n",
      "Iteration 714, loss = 0.03010040\n",
      "Iteration 715, loss = 0.03001001\n",
      "Iteration 716, loss = 0.02993385\n",
      "Iteration 717, loss = 0.02990605\n",
      "Iteration 718, loss = 0.02987791\n",
      "Iteration 719, loss = 0.02977782\n",
      "Iteration 720, loss = 0.02975274\n",
      "Iteration 721, loss = 0.02971085\n",
      "Iteration 722, loss = 0.02964330\n",
      "Iteration 723, loss = 0.02960353\n",
      "Iteration 724, loss = 0.02952369\n",
      "Iteration 725, loss = 0.02953636\n",
      "Iteration 726, loss = 0.02960009\n",
      "Iteration 727, loss = 0.02939108\n",
      "Iteration 728, loss = 0.02932157\n",
      "Iteration 729, loss = 0.02933811\n",
      "Iteration 730, loss = 0.02936990\n",
      "Iteration 731, loss = 0.02926359\n",
      "Iteration 732, loss = 0.02922826\n",
      "Iteration 733, loss = 0.02914814\n",
      "Iteration 734, loss = 0.02906736\n",
      "Iteration 735, loss = 0.02901770\n",
      "Iteration 736, loss = 0.02894489\n",
      "Iteration 737, loss = 0.02891714\n",
      "Iteration 738, loss = 0.02908159\n",
      "Iteration 739, loss = 0.02894933\n",
      "Iteration 740, loss = 0.02883157\n",
      "Iteration 741, loss = 0.02869849\n",
      "Iteration 742, loss = 0.02868943\n",
      "Iteration 743, loss = 0.02861723\n",
      "Iteration 744, loss = 0.02855522\n",
      "Iteration 745, loss = 0.02855290\n",
      "Iteration 746, loss = 0.02851871\n",
      "Iteration 747, loss = 0.02848003\n",
      "Iteration 748, loss = 0.02847162\n",
      "Iteration 749, loss = 0.02845266\n",
      "Iteration 750, loss = 0.02830577\n",
      "Iteration 751, loss = 0.02836007\n",
      "Iteration 752, loss = 0.02821440\n",
      "Iteration 753, loss = 0.02826881\n",
      "Iteration 754, loss = 0.02811175\n",
      "Iteration 755, loss = 0.02803967\n",
      "Iteration 756, loss = 0.02805546\n",
      "Iteration 757, loss = 0.02798922\n",
      "Iteration 758, loss = 0.02792798\n",
      "Iteration 759, loss = 0.02788886\n",
      "Iteration 760, loss = 0.02791370\n",
      "Iteration 761, loss = 0.02784700\n",
      "Iteration 762, loss = 0.02777854\n",
      "Iteration 763, loss = 0.02767503\n",
      "Iteration 764, loss = 0.02763758\n",
      "Iteration 765, loss = 0.02774141\n",
      "Iteration 766, loss = 0.02772844\n",
      "Iteration 767, loss = 0.02754173\n",
      "Iteration 768, loss = 0.02749888\n",
      "Iteration 769, loss = 0.02750587\n",
      "Iteration 770, loss = 0.02738536\n",
      "Iteration 771, loss = 0.02735599\n",
      "Iteration 772, loss = 0.02729905\n",
      "Iteration 773, loss = 0.02729794\n",
      "Iteration 774, loss = 0.02728694\n",
      "Iteration 775, loss = 0.02717746\n",
      "Iteration 776, loss = 0.02715578\n",
      "Iteration 777, loss = 0.02713031\n",
      "Iteration 778, loss = 0.02703208\n",
      "Iteration 779, loss = 0.02702166\n",
      "Iteration 780, loss = 0.02698351\n",
      "Iteration 781, loss = 0.02692481\n",
      "Iteration 782, loss = 0.02691232\n",
      "Iteration 783, loss = 0.02689786\n",
      "Iteration 784, loss = 0.02679900\n",
      "Iteration 785, loss = 0.02675247\n",
      "Iteration 786, loss = 0.02673746\n",
      "Iteration 787, loss = 0.02666544\n",
      "Iteration 788, loss = 0.02660957\n",
      "Iteration 789, loss = 0.02657629\n",
      "Iteration 790, loss = 0.02659242\n",
      "Iteration 791, loss = 0.02650034\n",
      "Iteration 792, loss = 0.02645325\n",
      "Iteration 793, loss = 0.02644424\n",
      "Iteration 794, loss = 0.02635874\n",
      "Iteration 795, loss = 0.02631626\n",
      "Iteration 796, loss = 0.02633271\n",
      "Iteration 797, loss = 0.02626039\n",
      "Iteration 798, loss = 0.02627475\n",
      "Iteration 799, loss = 0.02615873\n",
      "Iteration 800, loss = 0.02615796\n",
      "Iteration 801, loss = 0.02615966\n",
      "Iteration 802, loss = 0.02609960\n",
      "Iteration 803, loss = 0.02606933\n",
      "Iteration 804, loss = 0.02595212\n",
      "Iteration 805, loss = 0.02599072\n",
      "Iteration 806, loss = 0.02592668\n",
      "Iteration 807, loss = 0.02588606\n",
      "Iteration 808, loss = 0.02579471\n",
      "Iteration 809, loss = 0.02581038\n",
      "Iteration 810, loss = 0.02574935\n",
      "Iteration 811, loss = 0.02574155\n",
      "Iteration 812, loss = 0.02572283\n",
      "Iteration 813, loss = 0.02562269\n",
      "Iteration 814, loss = 0.02558111\n",
      "Iteration 815, loss = 0.02559529\n",
      "Iteration 816, loss = 0.02549652\n",
      "Iteration 817, loss = 0.02552956\n",
      "Iteration 818, loss = 0.02556683\n",
      "Iteration 819, loss = 0.02537962\n",
      "Iteration 820, loss = 0.02537201\n",
      "Iteration 821, loss = 0.02530125\n",
      "Iteration 822, loss = 0.02535626\n",
      "Iteration 823, loss = 0.02527610\n",
      "Iteration 824, loss = 0.02521559\n",
      "Iteration 825, loss = 0.02515112\n",
      "Iteration 826, loss = 0.02513512\n",
      "Iteration 827, loss = 0.02507704\n",
      "Iteration 828, loss = 0.02510836\n",
      "Iteration 829, loss = 0.02499648\n",
      "Iteration 830, loss = 0.02496963\n",
      "Iteration 831, loss = 0.02496481\n",
      "Iteration 832, loss = 0.02489755\n",
      "Iteration 833, loss = 0.02491824\n",
      "Iteration 834, loss = 0.02486590\n",
      "Iteration 835, loss = 0.02481999\n",
      "Iteration 836, loss = 0.02478521\n",
      "Iteration 837, loss = 0.02470192\n",
      "Iteration 838, loss = 0.02470743\n",
      "Iteration 839, loss = 0.02466327\n",
      "Iteration 840, loss = 0.02465229\n",
      "Iteration 841, loss = 0.02455216\n",
      "Iteration 842, loss = 0.02456310\n",
      "Iteration 843, loss = 0.02447751\n",
      "Iteration 844, loss = 0.02450768\n",
      "Iteration 845, loss = 0.02441998\n",
      "Iteration 846, loss = 0.02438154\n",
      "Iteration 847, loss = 0.02436954\n",
      "Iteration 848, loss = 0.02436316\n",
      "Iteration 849, loss = 0.02425359\n",
      "Iteration 850, loss = 0.02430648\n",
      "Iteration 851, loss = 0.02418947\n",
      "Iteration 852, loss = 0.02415256\n",
      "Iteration 853, loss = 0.02414871\n",
      "Iteration 854, loss = 0.02408073\n",
      "Iteration 855, loss = 0.02409868\n",
      "Iteration 856, loss = 0.02401999\n",
      "Iteration 857, loss = 0.02402870\n",
      "Iteration 858, loss = 0.02394911\n",
      "Iteration 859, loss = 0.02393227\n",
      "Iteration 860, loss = 0.02386894\n",
      "Iteration 861, loss = 0.02386951\n",
      "Iteration 862, loss = 0.02382081\n",
      "Iteration 863, loss = 0.02381486\n",
      "Iteration 864, loss = 0.02372951\n",
      "Iteration 865, loss = 0.02376424\n",
      "Iteration 866, loss = 0.02369067\n",
      "Iteration 867, loss = 0.02364104\n",
      "Iteration 868, loss = 0.02372204\n",
      "Iteration 869, loss = 0.02358660\n",
      "Iteration 870, loss = 0.02356425\n",
      "Iteration 871, loss = 0.02354426\n",
      "Iteration 872, loss = 0.02353094\n",
      "Iteration 873, loss = 0.02343436\n",
      "Iteration 874, loss = 0.02345138\n",
      "Iteration 875, loss = 0.02339740\n",
      "Iteration 876, loss = 0.02338264\n",
      "Iteration 877, loss = 0.02333691\n",
      "Iteration 878, loss = 0.02331393\n",
      "Iteration 879, loss = 0.02322414\n",
      "Iteration 880, loss = 0.02326927\n",
      "Iteration 881, loss = 0.02323646\n",
      "Iteration 882, loss = 0.02314403\n",
      "Iteration 883, loss = 0.02309759\n",
      "Iteration 884, loss = 0.02309323\n",
      "Iteration 885, loss = 0.02302930\n",
      "Iteration 886, loss = 0.02300907\n",
      "Iteration 887, loss = 0.02303776\n",
      "Iteration 888, loss = 0.02297256\n",
      "Iteration 889, loss = 0.02300170\n",
      "Iteration 890, loss = 0.02288681\n",
      "Iteration 891, loss = 0.02289802\n",
      "Iteration 892, loss = 0.02281847\n",
      "Iteration 893, loss = 0.02281552\n",
      "Iteration 894, loss = 0.02273929\n",
      "Iteration 895, loss = 0.02280239\n",
      "Iteration 896, loss = 0.02267601\n",
      "Iteration 897, loss = 0.02268230\n",
      "Iteration 898, loss = 0.02264262\n",
      "Iteration 899, loss = 0.02263526\n",
      "Iteration 900, loss = 0.02257459\n",
      "Iteration 901, loss = 0.02252572\n",
      "Iteration 902, loss = 0.02254932\n",
      "Iteration 903, loss = 0.02246519\n",
      "Iteration 904, loss = 0.02244232\n",
      "Iteration 905, loss = 0.02247066\n",
      "Iteration 906, loss = 0.02240012\n",
      "Iteration 907, loss = 0.02235590\n",
      "Iteration 908, loss = 0.02243861\n",
      "Iteration 909, loss = 0.02231676\n",
      "Iteration 910, loss = 0.02225720\n",
      "Iteration 911, loss = 0.02221007\n",
      "Iteration 912, loss = 0.02227189\n",
      "Iteration 913, loss = 0.02215370\n",
      "Iteration 914, loss = 0.02219063\n",
      "Iteration 915, loss = 0.02210424\n",
      "Iteration 916, loss = 0.02208136\n",
      "Iteration 917, loss = 0.02216514\n",
      "Iteration 918, loss = 0.02201102\n",
      "Iteration 919, loss = 0.02205169\n",
      "Iteration 920, loss = 0.02195271\n",
      "Iteration 921, loss = 0.02194725\n",
      "Iteration 922, loss = 0.02193134\n",
      "Iteration 923, loss = 0.02186679\n",
      "Iteration 924, loss = 0.02185739\n",
      "Iteration 925, loss = 0.02178787\n",
      "Iteration 926, loss = 0.02184571\n",
      "Iteration 927, loss = 0.02173742\n",
      "Iteration 928, loss = 0.02179471\n",
      "Iteration 929, loss = 0.02170470\n",
      "Iteration 930, loss = 0.02166154\n",
      "Iteration 931, loss = 0.02162105\n",
      "Iteration 932, loss = 0.02161823\n",
      "Iteration 933, loss = 0.02154714\n",
      "Iteration 934, loss = 0.02157787\n",
      "Iteration 935, loss = 0.02150816\n",
      "Iteration 936, loss = 0.02150972\n",
      "Iteration 937, loss = 0.02144840\n",
      "Iteration 938, loss = 0.02143866\n",
      "Iteration 939, loss = 0.02139681\n",
      "Iteration 940, loss = 0.02136186\n",
      "Iteration 941, loss = 0.02135942\n",
      "Iteration 942, loss = 0.02135152\n",
      "Iteration 943, loss = 0.02129662\n",
      "Iteration 944, loss = 0.02126444\n",
      "Iteration 945, loss = 0.02125246\n",
      "Iteration 946, loss = 0.02118468\n",
      "Iteration 947, loss = 0.02117466\n",
      "Iteration 948, loss = 0.02114639\n",
      "Iteration 949, loss = 0.02111229\n",
      "Iteration 950, loss = 0.02107726\n",
      "Iteration 951, loss = 0.02105589\n",
      "Iteration 952, loss = 0.02107774\n",
      "Iteration 953, loss = 0.02098753\n",
      "Iteration 954, loss = 0.02098402\n",
      "Iteration 955, loss = 0.02095684\n",
      "Iteration 956, loss = 0.02102660\n",
      "Iteration 957, loss = 0.02091354\n",
      "Iteration 958, loss = 0.02085407\n",
      "Iteration 959, loss = 0.02092696\n",
      "Iteration 960, loss = 0.02082424\n",
      "Iteration 961, loss = 0.02078918\n",
      "Iteration 962, loss = 0.02076115\n",
      "Iteration 963, loss = 0.02074027\n",
      "Iteration 964, loss = 0.02072633\n",
      "Iteration 965, loss = 0.02073737\n",
      "Iteration 966, loss = 0.02065712\n",
      "Iteration 967, loss = 0.02062879\n",
      "Iteration 968, loss = 0.02058460\n",
      "Iteration 969, loss = 0.02059891\n",
      "Iteration 970, loss = 0.02058125\n",
      "Iteration 971, loss = 0.02056963\n",
      "Iteration 972, loss = 0.02053111\n",
      "Iteration 973, loss = 0.02050082\n",
      "Iteration 974, loss = 0.02042124\n",
      "Iteration 975, loss = 0.02042894\n",
      "Iteration 976, loss = 0.02036744\n",
      "Iteration 977, loss = 0.02036766\n",
      "Iteration 978, loss = 0.02036635\n",
      "Iteration 979, loss = 0.02032650\n",
      "Iteration 980, loss = 0.02030345\n",
      "Iteration 981, loss = 0.02033828\n",
      "Iteration 982, loss = 0.02021796\n",
      "Iteration 983, loss = 0.02023767\n",
      "Iteration 984, loss = 0.02020379\n",
      "Iteration 985, loss = 0.02015441\n",
      "Iteration 986, loss = 0.02010685\n",
      "Iteration 987, loss = 0.02013930\n",
      "Iteration 988, loss = 0.02007732\n",
      "Iteration 989, loss = 0.02005698\n",
      "Iteration 990, loss = 0.02002229\n",
      "Iteration 991, loss = 0.01997594\n",
      "Iteration 992, loss = 0.02000549\n",
      "Iteration 993, loss = 0.01999665\n",
      "Iteration 994, loss = 0.01992284\n",
      "Iteration 995, loss = 0.01990230\n",
      "Iteration 996, loss = 0.01990616\n",
      "Iteration 997, loss = 0.01983046\n",
      "Iteration 998, loss = 0.01982570\n",
      "Iteration 999, loss = 0.01982678\n",
      "Iteration 1000, loss = 0.01976607\n",
      "Iteration 1001, loss = 0.01975198\n",
      "Iteration 1002, loss = 0.01970742\n",
      "Iteration 1003, loss = 0.01969183\n",
      "Iteration 1004, loss = 0.01967271\n",
      "Iteration 1005, loss = 0.01968084\n",
      "Iteration 1006, loss = 0.01970465\n",
      "Iteration 1007, loss = 0.01961891\n",
      "Iteration 1008, loss = 0.01956807\n",
      "Iteration 1009, loss = 0.01957712\n",
      "Iteration 1010, loss = 0.01951360\n",
      "Iteration 1011, loss = 0.01954273\n",
      "Iteration 1012, loss = 0.01947705\n",
      "Iteration 1013, loss = 0.01948064\n",
      "Iteration 1014, loss = 0.01946171\n",
      "Iteration 1015, loss = 0.01939916\n",
      "Iteration 1016, loss = 0.01937597\n",
      "Iteration 1017, loss = 0.01937547\n",
      "Iteration 1018, loss = 0.01933427\n",
      "Iteration 1019, loss = 0.01929603\n",
      "Iteration 1020, loss = 0.01928234\n",
      "Iteration 1021, loss = 0.01925113\n",
      "Iteration 1022, loss = 0.01931297\n",
      "Iteration 1023, loss = 0.01919687\n",
      "Iteration 1024, loss = 0.01916827\n",
      "Iteration 1025, loss = 0.01915250\n",
      "Iteration 1026, loss = 0.01912024\n",
      "Iteration 1027, loss = 0.01910287\n",
      "Iteration 1028, loss = 0.01909286\n",
      "Iteration 1029, loss = 0.01907039\n",
      "Iteration 1030, loss = 0.01903364\n",
      "Iteration 1031, loss = 0.01900740\n",
      "Iteration 1032, loss = 0.01899989\n",
      "Iteration 1033, loss = 0.01896073\n",
      "Iteration 1034, loss = 0.01895706\n",
      "Iteration 1035, loss = 0.01896038\n",
      "Iteration 1036, loss = 0.01897234\n",
      "Iteration 1037, loss = 0.01891119\n",
      "Iteration 1038, loss = 0.01890476\n",
      "Iteration 1039, loss = 0.01882657\n",
      "Iteration 1040, loss = 0.01880322\n",
      "Iteration 1041, loss = 0.01881173\n",
      "Iteration 1042, loss = 0.01878207\n",
      "Iteration 1043, loss = 0.01879581\n",
      "Iteration 1044, loss = 0.01872774\n",
      "Iteration 1045, loss = 0.01876881\n",
      "Iteration 1046, loss = 0.01868355\n",
      "Iteration 1047, loss = 0.01866664\n",
      "Iteration 1048, loss = 0.01862782\n",
      "Iteration 1049, loss = 0.01860726\n",
      "Iteration 1050, loss = 0.01859899\n",
      "Iteration 1051, loss = 0.01863936\n",
      "Iteration 1052, loss = 0.01853527\n",
      "Iteration 1053, loss = 0.01851493\n",
      "Iteration 1054, loss = 0.01860427\n",
      "Iteration 1055, loss = 0.01846745\n",
      "Iteration 1056, loss = 0.01846008\n",
      "Iteration 1057, loss = 0.01843632\n",
      "Iteration 1058, loss = 0.01841705\n",
      "Iteration 1059, loss = 0.01838588\n",
      "Iteration 1060, loss = 0.01836636\n",
      "Iteration 1061, loss = 0.01833006\n",
      "Iteration 1062, loss = 0.01835318\n",
      "Iteration 1063, loss = 0.01830015\n",
      "Iteration 1064, loss = 0.01827471\n",
      "Iteration 1065, loss = 0.01824368\n",
      "Iteration 1066, loss = 0.01823263\n",
      "Iteration 1067, loss = 0.01827208\n",
      "Iteration 1068, loss = 0.01825369\n",
      "Iteration 1069, loss = 0.01817340\n",
      "Iteration 1070, loss = 0.01817305\n",
      "Iteration 1071, loss = 0.01816284\n",
      "Iteration 1072, loss = 0.01810236\n",
      "Iteration 1073, loss = 0.01813643\n",
      "Iteration 1074, loss = 0.01806346\n",
      "Iteration 1075, loss = 0.01813347\n",
      "Iteration 1076, loss = 0.01804640\n",
      "Iteration 1077, loss = 0.01800230\n",
      "Iteration 1078, loss = 0.01802085\n",
      "Iteration 1079, loss = 0.01795471\n",
      "Iteration 1080, loss = 0.01792085\n",
      "Iteration 1081, loss = 0.01792370\n",
      "Iteration 1082, loss = 0.01788427\n",
      "Iteration 1083, loss = 0.01791740\n",
      "Iteration 1084, loss = 0.01789137\n",
      "Iteration 1085, loss = 0.01782512\n",
      "Iteration 1086, loss = 0.01779992\n",
      "Iteration 1087, loss = 0.01779303\n",
      "Iteration 1088, loss = 0.01779889\n",
      "Iteration 1089, loss = 0.01773319\n",
      "Iteration 1090, loss = 0.01772852\n",
      "Iteration 1091, loss = 0.01770594\n",
      "Iteration 1092, loss = 0.01767895\n",
      "Iteration 1093, loss = 0.01766710\n",
      "Iteration 1094, loss = 0.01763058\n",
      "Iteration 1095, loss = 0.01763203\n",
      "Iteration 1096, loss = 0.01763486\n",
      "Iteration 1097, loss = 0.01758546\n",
      "Iteration 1098, loss = 0.01765230\n",
      "Iteration 1099, loss = 0.01757566\n",
      "Iteration 1100, loss = 0.01755386\n",
      "Iteration 1101, loss = 0.01755203\n",
      "Iteration 1102, loss = 0.01749445\n",
      "Iteration 1103, loss = 0.01745182\n",
      "Iteration 1104, loss = 0.01746202\n",
      "Iteration 1105, loss = 0.01745254\n",
      "Iteration 1106, loss = 0.01744385\n",
      "Iteration 1107, loss = 0.01740772\n",
      "Iteration 1108, loss = 0.01735452\n",
      "Iteration 1109, loss = 0.01733067\n",
      "Iteration 1110, loss = 0.01734174\n",
      "Iteration 1111, loss = 0.01735577\n",
      "Iteration 1112, loss = 0.01728300\n",
      "Iteration 1113, loss = 0.01730846\n",
      "Iteration 1114, loss = 0.01723603\n",
      "Iteration 1115, loss = 0.01721951\n",
      "Iteration 1116, loss = 0.01719639\n",
      "Iteration 1117, loss = 0.01718260\n",
      "Iteration 1118, loss = 0.01717618\n",
      "Iteration 1119, loss = 0.01712536\n",
      "Iteration 1120, loss = 0.01711268\n",
      "Iteration 1121, loss = 0.01711887\n",
      "Iteration 1122, loss = 0.01706891\n",
      "Iteration 1123, loss = 0.01706727\n",
      "Iteration 1124, loss = 0.01704744\n",
      "Iteration 1125, loss = 0.01706351\n",
      "Iteration 1126, loss = 0.01702841\n",
      "Iteration 1127, loss = 0.01699383\n",
      "Iteration 1128, loss = 0.01698442\n",
      "Iteration 1129, loss = 0.01694315\n",
      "Iteration 1130, loss = 0.01695473\n",
      "Iteration 1131, loss = 0.01694288\n",
      "Iteration 1132, loss = 0.01689703\n",
      "Iteration 1133, loss = 0.01687517\n",
      "Iteration 1134, loss = 0.01688726\n",
      "Iteration 1135, loss = 0.01683122\n",
      "Iteration 1136, loss = 0.01684839\n",
      "Iteration 1137, loss = 0.01678675\n",
      "Iteration 1138, loss = 0.01679820\n",
      "Iteration 1139, loss = 0.01676115\n",
      "Iteration 1140, loss = 0.01678111\n",
      "Iteration 1141, loss = 0.01673974\n",
      "Iteration 1142, loss = 0.01670356\n",
      "Iteration 1143, loss = 0.01670040\n",
      "Iteration 1144, loss = 0.01671062\n",
      "Iteration 1145, loss = 0.01668583\n",
      "Iteration 1146, loss = 0.01668974\n",
      "Iteration 1147, loss = 0.01662308\n",
      "Iteration 1148, loss = 0.01663966\n",
      "Iteration 1149, loss = 0.01666570\n",
      "Iteration 1150, loss = 0.01655998\n",
      "Iteration 1151, loss = 0.01654034\n",
      "Iteration 1152, loss = 0.01651833\n",
      "Iteration 1153, loss = 0.01648800\n",
      "Iteration 1154, loss = 0.01649274\n",
      "Iteration 1155, loss = 0.01648167\n",
      "Iteration 1156, loss = 0.01646680\n",
      "Iteration 1157, loss = 0.01642647\n",
      "Iteration 1158, loss = 0.01644143\n",
      "Iteration 1159, loss = 0.01637121\n",
      "Iteration 1160, loss = 0.01636788\n",
      "Iteration 1161, loss = 0.01637300\n",
      "Iteration 1162, loss = 0.01633995\n",
      "Iteration 1163, loss = 0.01631281\n",
      "Iteration 1164, loss = 0.01630176\n",
      "Iteration 1165, loss = 0.01629385\n",
      "Iteration 1166, loss = 0.01632013\n",
      "Iteration 1167, loss = 0.01624833\n",
      "Iteration 1168, loss = 0.01621872\n",
      "Iteration 1169, loss = 0.01621474\n",
      "Iteration 1170, loss = 0.01627401\n",
      "Iteration 1171, loss = 0.01615812\n",
      "Iteration 1172, loss = 0.01615869\n",
      "Iteration 1173, loss = 0.01616018\n",
      "Iteration 1174, loss = 0.01612355\n",
      "Iteration 1175, loss = 0.01609908\n",
      "Iteration 1176, loss = 0.01613594\n",
      "Iteration 1177, loss = 0.01609401\n",
      "Iteration 1178, loss = 0.01604475\n",
      "Iteration 1179, loss = 0.01603781\n",
      "Iteration 1180, loss = 0.01606558\n",
      "Iteration 1181, loss = 0.01600834\n",
      "Iteration 1182, loss = 0.01605621\n",
      "Iteration 1183, loss = 0.01596579\n",
      "Iteration 1184, loss = 0.01601396\n",
      "Iteration 1185, loss = 0.01595431\n",
      "Iteration 1186, loss = 0.01591832\n",
      "Iteration 1187, loss = 0.01589609\n",
      "Iteration 1188, loss = 0.01589997\n",
      "Iteration 1189, loss = 0.01588527\n",
      "Iteration 1190, loss = 0.01584799\n",
      "Iteration 1191, loss = 0.01582661\n",
      "Iteration 1192, loss = 0.01581648\n",
      "Iteration 1193, loss = 0.01579821\n",
      "Iteration 1194, loss = 0.01582468\n",
      "Iteration 1195, loss = 0.01576723\n",
      "Iteration 1196, loss = 0.01575580\n",
      "Iteration 1197, loss = 0.01572640\n",
      "Iteration 1198, loss = 0.01576405\n",
      "Iteration 1199, loss = 0.01569361\n",
      "Iteration 1200, loss = 0.01569391\n",
      "Iteration 1201, loss = 0.01567443\n",
      "Iteration 1202, loss = 0.01563954\n",
      "Iteration 1203, loss = 0.01564611\n",
      "Iteration 1204, loss = 0.01562925\n",
      "Iteration 1205, loss = 0.01560628\n",
      "Iteration 1206, loss = 0.01563875\n",
      "Iteration 1207, loss = 0.01560819\n",
      "Iteration 1208, loss = 0.01559375\n",
      "Iteration 1209, loss = 0.01552302\n",
      "Iteration 1210, loss = 0.01555803\n",
      "Iteration 1211, loss = 0.01551514\n",
      "Iteration 1212, loss = 0.01551656\n",
      "Iteration 1213, loss = 0.01546739\n",
      "Iteration 1214, loss = 0.01545685\n",
      "Iteration 1215, loss = 0.01543521\n",
      "Iteration 1216, loss = 0.01541615\n",
      "Iteration 1217, loss = 0.01539788\n",
      "Iteration 1218, loss = 0.01538488\n",
      "Iteration 1219, loss = 0.01539611\n",
      "Iteration 1220, loss = 0.01534536\n",
      "Iteration 1221, loss = 0.01535813\n",
      "Iteration 1222, loss = 0.01531379\n",
      "Iteration 1223, loss = 0.01537717\n",
      "Iteration 1224, loss = 0.01529802\n",
      "Iteration 1225, loss = 0.01526860\n",
      "Iteration 1226, loss = 0.01528682\n",
      "Iteration 1227, loss = 0.01524875\n",
      "Iteration 1228, loss = 0.01522144\n",
      "Iteration 1229, loss = 0.01527642\n",
      "Iteration 1230, loss = 0.01518793\n",
      "Iteration 1231, loss = 0.01519364\n",
      "Iteration 1232, loss = 0.01519620\n",
      "Iteration 1233, loss = 0.01515503\n",
      "Iteration 1234, loss = 0.01514535\n",
      "Iteration 1235, loss = 0.01512113\n",
      "Iteration 1236, loss = 0.01518779\n",
      "Iteration 1237, loss = 0.01507901\n",
      "Iteration 1238, loss = 0.01508430\n",
      "Iteration 1239, loss = 0.01505081\n",
      "Iteration 1240, loss = 0.01505161\n",
      "Iteration 1241, loss = 0.01504943\n",
      "Iteration 1242, loss = 0.01500432\n",
      "Iteration 1243, loss = 0.01498379\n",
      "Iteration 1244, loss = 0.01497439\n",
      "Iteration 1245, loss = 0.01497689\n",
      "Iteration 1246, loss = 0.01494200\n",
      "Iteration 1247, loss = 0.01494473\n",
      "Iteration 1248, loss = 0.01492068\n",
      "Iteration 1249, loss = 0.01491245\n",
      "Iteration 1250, loss = 0.01487777\n",
      "Iteration 1251, loss = 0.01487517\n",
      "Iteration 1252, loss = 0.01492475\n",
      "Iteration 1253, loss = 0.01487394\n",
      "Iteration 1254, loss = 0.01482905\n",
      "Iteration 1255, loss = 0.01492323\n",
      "Iteration 1256, loss = 0.01478301\n",
      "Iteration 1257, loss = 0.01479725\n",
      "Iteration 1258, loss = 0.01476858\n",
      "Iteration 1259, loss = 0.01473909\n",
      "Iteration 1260, loss = 0.01473433\n",
      "Iteration 1261, loss = 0.01471376\n",
      "Iteration 1262, loss = 0.01471835\n",
      "Iteration 1263, loss = 0.01473039\n",
      "Iteration 1264, loss = 0.01470660\n",
      "Iteration 1265, loss = 0.01464662\n",
      "Iteration 1266, loss = 0.01465744\n",
      "Iteration 1267, loss = 0.01462444\n",
      "Iteration 1268, loss = 0.01461604\n",
      "Iteration 1269, loss = 0.01459586\n",
      "Iteration 1270, loss = 0.01457858\n",
      "Iteration 1271, loss = 0.01459281\n",
      "Iteration 1272, loss = 0.01455219\n",
      "Iteration 1273, loss = 0.01456515\n",
      "Iteration 1274, loss = 0.01455114\n",
      "Iteration 1275, loss = 0.01451375\n",
      "Iteration 1276, loss = 0.01450873\n",
      "Iteration 1277, loss = 0.01449027\n",
      "Iteration 1278, loss = 0.01446988\n",
      "Iteration 1279, loss = 0.01449150\n",
      "Iteration 1280, loss = 0.01443443\n",
      "Iteration 1281, loss = 0.01447245\n",
      "Iteration 1282, loss = 0.01440959\n",
      "Iteration 1283, loss = 0.01439926\n",
      "Iteration 1284, loss = 0.01439807\n",
      "Iteration 1285, loss = 0.01437623\n",
      "Iteration 1286, loss = 0.01437501\n",
      "Iteration 1287, loss = 0.01433574\n",
      "Iteration 1288, loss = 0.01433355\n",
      "Iteration 1289, loss = 0.01433518\n",
      "Iteration 1290, loss = 0.01430174\n",
      "Iteration 1291, loss = 0.01430347\n",
      "Iteration 1292, loss = 0.01427118\n",
      "Iteration 1293, loss = 0.01428557\n",
      "Iteration 1294, loss = 0.01424010\n",
      "Iteration 1295, loss = 0.01422571\n",
      "Iteration 1296, loss = 0.01420103\n",
      "Iteration 1297, loss = 0.01418861\n",
      "Iteration 1298, loss = 0.01418449\n",
      "Iteration 1299, loss = 0.01418932\n",
      "Iteration 1300, loss = 0.01415846\n",
      "Iteration 1301, loss = 0.01414088\n",
      "Iteration 1302, loss = 0.01423305\n",
      "Iteration 1303, loss = 0.01413156\n",
      "Iteration 1304, loss = 0.01409392\n",
      "Iteration 1305, loss = 0.01408769\n",
      "Iteration 1306, loss = 0.01407016\n",
      "Iteration 1307, loss = 0.01408970\n",
      "Iteration 1308, loss = 0.01407395\n",
      "Iteration 1309, loss = 0.01403935\n",
      "Iteration 1310, loss = 0.01403673\n",
      "Iteration 1311, loss = 0.01399989\n",
      "Iteration 1312, loss = 0.01399016\n",
      "Iteration 1313, loss = 0.01403545\n",
      "Iteration 1314, loss = 0.01400561\n",
      "Iteration 1315, loss = 0.01393618\n",
      "Iteration 1316, loss = 0.01394179\n",
      "Iteration 1317, loss = 0.01392310\n",
      "Iteration 1318, loss = 0.01394186\n",
      "Iteration 1319, loss = 0.01391147\n",
      "Iteration 1320, loss = 0.01387764\n",
      "Iteration 1321, loss = 0.01392189\n",
      "Iteration 1322, loss = 0.01384188\n",
      "Iteration 1323, loss = 0.01384474\n",
      "Iteration 1324, loss = 0.01384157\n",
      "Iteration 1325, loss = 0.01381568\n",
      "Iteration 1326, loss = 0.01380081\n",
      "Iteration 1327, loss = 0.01379115\n",
      "Iteration 1328, loss = 0.01377674\n",
      "Iteration 1329, loss = 0.01375878\n",
      "Iteration 1330, loss = 0.01376155\n",
      "Iteration 1331, loss = 0.01374882\n",
      "Iteration 1332, loss = 0.01371123\n",
      "Iteration 1333, loss = 0.01370951\n",
      "Iteration 1334, loss = 0.01370898\n",
      "Iteration 1335, loss = 0.01370972\n",
      "Iteration 1336, loss = 0.01366055\n",
      "Iteration 1337, loss = 0.01365423\n",
      "Iteration 1338, loss = 0.01365648\n",
      "Iteration 1339, loss = 0.01368772\n",
      "Iteration 1340, loss = 0.01363456\n",
      "Iteration 1341, loss = 0.01363477\n",
      "Iteration 1342, loss = 0.01359205\n",
      "Iteration 1343, loss = 0.01358081\n",
      "Iteration 1344, loss = 0.01357957\n",
      "Iteration 1345, loss = 0.01356643\n",
      "Iteration 1346, loss = 0.01353889\n",
      "Iteration 1347, loss = 0.01353296\n",
      "Iteration 1348, loss = 0.01351230\n",
      "Iteration 1349, loss = 0.01349429\n",
      "Iteration 1350, loss = 0.01349519\n",
      "Iteration 1351, loss = 0.01347123\n",
      "Iteration 1352, loss = 0.01345656\n",
      "Iteration 1353, loss = 0.01345110\n",
      "Iteration 1354, loss = 0.01345067\n",
      "Iteration 1355, loss = 0.01343529\n",
      "Iteration 1356, loss = 0.01341355\n",
      "Iteration 1357, loss = 0.01340911\n",
      "Iteration 1358, loss = 0.01338582\n",
      "Iteration 1359, loss = 0.01337081\n",
      "Iteration 1360, loss = 0.01336291\n",
      "Iteration 1361, loss = 0.01343342\n",
      "Iteration 1362, loss = 0.01332323\n",
      "Iteration 1363, loss = 0.01332921\n",
      "Iteration 1364, loss = 0.01330865\n",
      "Iteration 1365, loss = 0.01330359\n",
      "Iteration 1366, loss = 0.01329757\n",
      "Iteration 1367, loss = 0.01326322\n",
      "Iteration 1368, loss = 0.01327462\n",
      "Iteration 1369, loss = 0.01325866\n",
      "Iteration 1370, loss = 0.01322457\n",
      "Iteration 1371, loss = 0.01323842\n",
      "Iteration 1372, loss = 0.01322102\n",
      "Iteration 1373, loss = 0.01319982\n",
      "Iteration 1374, loss = 0.01318092\n",
      "Iteration 1375, loss = 0.01318025\n",
      "Iteration 1376, loss = 0.01322742\n",
      "Iteration 1377, loss = 0.01313659\n",
      "Iteration 1378, loss = 0.01313906\n",
      "Iteration 1379, loss = 0.01312472\n",
      "Iteration 1380, loss = 0.01310203\n",
      "Iteration 1381, loss = 0.01309279\n",
      "Iteration 1382, loss = 0.01311364\n",
      "Iteration 1383, loss = 0.01309525\n",
      "Iteration 1384, loss = 0.01305215\n",
      "Iteration 1385, loss = 0.01310086\n",
      "Iteration 1386, loss = 0.01308584\n",
      "Iteration 1387, loss = 0.01303064\n",
      "Iteration 1388, loss = 0.01299982\n",
      "Iteration 1389, loss = 0.01299055\n",
      "Iteration 1390, loss = 0.01298872\n",
      "Iteration 1391, loss = 0.01298357\n",
      "Iteration 1392, loss = 0.01296916\n",
      "Iteration 1393, loss = 0.01296042\n",
      "Iteration 1394, loss = 0.01294862\n",
      "Iteration 1395, loss = 0.01292582\n",
      "Iteration 1396, loss = 0.01291551\n",
      "Iteration 1397, loss = 0.01290948\n",
      "Iteration 1398, loss = 0.01289164\n",
      "Iteration 1399, loss = 0.01293265\n",
      "Iteration 1400, loss = 0.01286335\n",
      "Iteration 1401, loss = 0.01286439\n",
      "Iteration 1402, loss = 0.01284042\n",
      "Iteration 1403, loss = 0.01283544\n",
      "Iteration 1404, loss = 0.01281339\n",
      "Iteration 1405, loss = 0.01280130\n",
      "Iteration 1406, loss = 0.01282637\n",
      "Iteration 1407, loss = 0.01280836\n",
      "Iteration 1408, loss = 0.01278365\n",
      "Iteration 1409, loss = 0.01275839\n",
      "Iteration 1410, loss = 0.01275120\n",
      "Iteration 1411, loss = 0.01276799\n",
      "Iteration 1412, loss = 0.01275376\n",
      "Iteration 1413, loss = 0.01273809\n",
      "Iteration 1414, loss = 0.01270131\n",
      "Iteration 1415, loss = 0.01270388\n",
      "Iteration 1416, loss = 0.01269834\n",
      "Iteration 1417, loss = 0.01268072\n",
      "Iteration 1418, loss = 0.01266350\n",
      "Iteration 1419, loss = 0.01267263\n",
      "Iteration 1420, loss = 0.01268732\n",
      "Iteration 1421, loss = 0.01262173\n",
      "Iteration 1422, loss = 0.01265224\n",
      "Iteration 1423, loss = 0.01259346\n",
      "Iteration 1424, loss = 0.01261089\n",
      "Iteration 1425, loss = 0.01258021\n",
      "Iteration 1426, loss = 0.01257356\n",
      "Iteration 1427, loss = 0.01255392\n",
      "Iteration 1428, loss = 0.01255161\n",
      "Iteration 1429, loss = 0.01252406\n",
      "Iteration 1430, loss = 0.01252298\n",
      "Iteration 1431, loss = 0.01255105\n",
      "Iteration 1432, loss = 0.01250799\n",
      "Iteration 1433, loss = 0.01247928\n",
      "Iteration 1434, loss = 0.01248066\n",
      "Iteration 1435, loss = 0.01245887\n",
      "Iteration 1436, loss = 0.01245284\n",
      "Iteration 1437, loss = 0.01244304\n",
      "Iteration 1438, loss = 0.01244325\n",
      "Iteration 1439, loss = 0.01241348\n",
      "Iteration 1440, loss = 0.01242435\n",
      "Iteration 1441, loss = 0.01241085\n",
      "Iteration 1442, loss = 0.01239234\n",
      "Iteration 1443, loss = 0.01237423\n",
      "Iteration 1444, loss = 0.01236530\n",
      "Iteration 1445, loss = 0.01237746\n",
      "Iteration 1446, loss = 0.01233873\n",
      "Iteration 1447, loss = 0.01233508\n",
      "Iteration 1448, loss = 0.01236473\n",
      "Iteration 1449, loss = 0.01232682\n",
      "Iteration 1450, loss = 0.01229110\n",
      "Iteration 1451, loss = 0.01228732\n",
      "Iteration 1452, loss = 0.01227418\n",
      "Iteration 1453, loss = 0.01226284\n",
      "Iteration 1454, loss = 0.01224965\n",
      "Iteration 1455, loss = 0.01229035\n",
      "Iteration 1456, loss = 0.01223375\n",
      "Iteration 1457, loss = 0.01224467\n",
      "Iteration 1458, loss = 0.01220390\n",
      "Iteration 1459, loss = 0.01220091\n",
      "Iteration 1460, loss = 0.01219263\n",
      "Iteration 1461, loss = 0.01217705\n",
      "Iteration 1462, loss = 0.01217035\n",
      "Iteration 1463, loss = 0.01215414\n",
      "Iteration 1464, loss = 0.01215237\n",
      "Iteration 1465, loss = 0.01215807\n",
      "Iteration 1466, loss = 0.01212573\n",
      "Iteration 1467, loss = 0.01210811\n",
      "Iteration 1468, loss = 0.01211476\n",
      "Iteration 1469, loss = 0.01212216\n",
      "Iteration 1470, loss = 0.01210165\n",
      "Iteration 1471, loss = 0.01206592\n",
      "Iteration 1472, loss = 0.01207135\n",
      "Iteration 1473, loss = 0.01205667\n",
      "Iteration 1474, loss = 0.01204929\n",
      "Iteration 1475, loss = 0.01202886\n",
      "Iteration 1476, loss = 0.01202242\n",
      "Iteration 1477, loss = 0.01202484\n",
      "Iteration 1478, loss = 0.01201255\n",
      "Iteration 1479, loss = 0.01202949\n",
      "Iteration 1480, loss = 0.01199579\n",
      "Iteration 1481, loss = 0.01198844\n",
      "Iteration 1482, loss = 0.01195268\n",
      "Iteration 1483, loss = 0.01194609\n",
      "Iteration 1484, loss = 0.01194501\n",
      "Iteration 1485, loss = 0.01192739\n",
      "Iteration 1486, loss = 0.01196858\n",
      "Iteration 1487, loss = 0.01190155\n",
      "Iteration 1488, loss = 0.01189963\n",
      "Iteration 1489, loss = 0.01190610\n",
      "Iteration 1490, loss = 0.01187183\n",
      "Iteration 1491, loss = 0.01187664\n",
      "Iteration 1492, loss = 0.01185869\n",
      "Iteration 1493, loss = 0.01184925\n",
      "Iteration 1494, loss = 0.01186244\n",
      "Iteration 1495, loss = 0.01185024\n",
      "Iteration 1496, loss = 0.01181390\n",
      "Iteration 1497, loss = 0.01180845\n",
      "Iteration 1498, loss = 0.01179231\n",
      "Iteration 1499, loss = 0.01177878\n",
      "Iteration 1500, loss = 0.01179522\n",
      "Iteration 1501, loss = 0.01178913\n",
      "Iteration 1502, loss = 0.01178489\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "nnmlpsgd = mlpsgd.fit(trainData,trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.62688460\n",
      "Iteration 2, loss = 1.14959027\n",
      "Iteration 3, loss = 0.90961867\n",
      "Iteration 4, loss = 0.73129876\n",
      "Iteration 5, loss = 0.62314375\n",
      "Iteration 6, loss = 0.52367622\n",
      "Iteration 7, loss = 0.45641419\n",
      "Iteration 8, loss = 0.40844466\n",
      "Iteration 9, loss = 0.35610976\n",
      "Iteration 10, loss = 0.31346928\n",
      "Iteration 11, loss = 0.28161268\n",
      "Iteration 12, loss = 0.25771490\n",
      "Iteration 13, loss = 0.23431745\n",
      "Iteration 14, loss = 0.21182140\n",
      "Iteration 15, loss = 0.19377410\n",
      "Iteration 16, loss = 0.18173214\n",
      "Iteration 17, loss = 0.16733580\n",
      "Iteration 18, loss = 0.15633583\n",
      "Iteration 19, loss = 0.14616211\n",
      "Iteration 20, loss = 0.13710070\n",
      "Iteration 21, loss = 0.12694850\n",
      "Iteration 22, loss = 0.11908971\n",
      "Iteration 23, loss = 0.11283344\n",
      "Iteration 24, loss = 0.10783566\n",
      "Iteration 25, loss = 0.10760019\n",
      "Iteration 26, loss = 0.09992349\n",
      "Iteration 27, loss = 0.09205273\n",
      "Iteration 28, loss = 0.08815590\n",
      "Iteration 29, loss = 0.08291700\n",
      "Iteration 30, loss = 0.08178986\n",
      "Iteration 31, loss = 0.07517238\n",
      "Iteration 32, loss = 0.07350919\n",
      "Iteration 33, loss = 0.06910102\n",
      "Iteration 34, loss = 0.06558854\n",
      "Iteration 35, loss = 0.06597237\n",
      "Iteration 36, loss = 0.06564785\n",
      "Iteration 37, loss = 0.06289586\n",
      "Iteration 38, loss = 0.05844508\n",
      "Iteration 39, loss = 0.05311902\n",
      "Iteration 40, loss = 0.05051846\n",
      "Iteration 41, loss = 0.04934121\n",
      "Iteration 42, loss = 0.05091560\n",
      "Iteration 43, loss = 0.04820568\n",
      "Iteration 44, loss = 0.04647134\n",
      "Iteration 45, loss = 0.04374382\n",
      "Iteration 46, loss = 0.04115332\n",
      "Iteration 47, loss = 0.03939500\n",
      "Iteration 48, loss = 0.03869816\n",
      "Iteration 49, loss = 0.03734983\n",
      "Iteration 50, loss = 0.03669922\n",
      "Iteration 51, loss = 0.03510111\n",
      "Iteration 52, loss = 0.03351867\n",
      "Iteration 53, loss = 0.03340480\n",
      "Iteration 54, loss = 0.03189683\n",
      "Iteration 55, loss = 0.03166439\n",
      "Iteration 56, loss = 0.03049727\n",
      "Iteration 57, loss = 0.02950227\n",
      "Iteration 58, loss = 0.02874454\n",
      "Iteration 59, loss = 0.02745840\n",
      "Iteration 60, loss = 0.02700787\n",
      "Iteration 61, loss = 0.02611644\n",
      "Iteration 62, loss = 0.02515063\n",
      "Iteration 63, loss = 0.02459413\n",
      "Iteration 64, loss = 0.02397488\n",
      "Iteration 65, loss = 0.02315083\n",
      "Iteration 66, loss = 0.02331461\n",
      "Iteration 67, loss = 0.02330597\n",
      "Iteration 68, loss = 0.02191283\n",
      "Iteration 69, loss = 0.02157400\n",
      "Iteration 70, loss = 0.02002871\n",
      "Iteration 71, loss = 0.01977320\n",
      "Iteration 72, loss = 0.01925113\n",
      "Iteration 73, loss = 0.01901458\n",
      "Iteration 74, loss = 0.01848122\n",
      "Iteration 75, loss = 0.01762333\n",
      "Iteration 76, loss = 0.01720441\n",
      "Iteration 77, loss = 0.01685430\n",
      "Iteration 78, loss = 0.01708967\n",
      "Iteration 79, loss = 0.01744936\n",
      "Iteration 80, loss = 0.01615527\n",
      "Iteration 81, loss = 0.01522943\n",
      "Iteration 82, loss = 0.01440143\n",
      "Iteration 83, loss = 0.01413220\n",
      "Iteration 84, loss = 0.01375417\n",
      "Iteration 85, loss = 0.01339578\n",
      "Iteration 86, loss = 0.01339352\n",
      "Iteration 87, loss = 0.01276706\n",
      "Iteration 88, loss = 0.01250867\n",
      "Iteration 89, loss = 0.01223055\n",
      "Iteration 90, loss = 0.01193890\n",
      "Iteration 91, loss = 0.01174021\n",
      "Iteration 92, loss = 0.01146129\n",
      "Iteration 93, loss = 0.01147775\n",
      "Iteration 94, loss = 0.01103898\n",
      "Iteration 95, loss = 0.01089065\n",
      "Iteration 96, loss = 0.01044614\n",
      "Iteration 97, loss = 0.01017097\n",
      "Iteration 98, loss = 0.01012975\n",
      "Iteration 99, loss = 0.00974652\n",
      "Iteration 100, loss = 0.00969432\n",
      "Iteration 101, loss = 0.00956038\n",
      "Iteration 102, loss = 0.00933276\n",
      "Iteration 103, loss = 0.00926378\n",
      "Iteration 104, loss = 0.00881231\n",
      "Iteration 105, loss = 0.00855445\n",
      "Iteration 106, loss = 0.00869091\n",
      "Iteration 107, loss = 0.00822339\n",
      "Iteration 108, loss = 0.00832964\n",
      "Iteration 109, loss = 0.00785482\n",
      "Iteration 110, loss = 0.00772468\n",
      "Iteration 111, loss = 0.00776488\n",
      "Iteration 112, loss = 0.00746437\n",
      "Iteration 113, loss = 0.00747893\n",
      "Iteration 114, loss = 0.00741551\n",
      "Iteration 115, loss = 0.00710541\n",
      "Iteration 116, loss = 0.00696509\n",
      "Iteration 117, loss = 0.00688008\n",
      "Iteration 118, loss = 0.00676654\n",
      "Iteration 119, loss = 0.00656154\n",
      "Iteration 120, loss = 0.00644870\n",
      "Iteration 121, loss = 0.00635520\n",
      "Iteration 122, loss = 0.00633767\n",
      "Iteration 123, loss = 0.00620389\n",
      "Iteration 124, loss = 0.00602860\n",
      "Iteration 125, loss = 0.00633395\n",
      "Iteration 126, loss = 0.00616009\n",
      "Iteration 127, loss = 0.00582103\n",
      "Iteration 128, loss = 0.00550852\n",
      "Iteration 129, loss = 0.00553920\n",
      "Iteration 130, loss = 0.00547382\n",
      "Iteration 131, loss = 0.00548106\n",
      "Iteration 132, loss = 0.00577396\n",
      "Iteration 133, loss = 0.00527759\n",
      "Iteration 134, loss = 0.00521901\n",
      "Iteration 135, loss = 0.00499877\n",
      "Iteration 136, loss = 0.00513419\n",
      "Iteration 137, loss = 0.00480966\n",
      "Iteration 138, loss = 0.00477938\n",
      "Iteration 139, loss = 0.00475803\n",
      "Iteration 140, loss = 0.00464757\n",
      "Iteration 141, loss = 0.00453909\n",
      "Iteration 142, loss = 0.00444940\n",
      "Iteration 143, loss = 0.00445116\n",
      "Iteration 144, loss = 0.00439142\n",
      "Iteration 145, loss = 0.00447437\n",
      "Iteration 146, loss = 0.00422336\n",
      "Iteration 147, loss = 0.00418000\n",
      "Iteration 148, loss = 0.00405258\n",
      "Iteration 149, loss = 0.00405874\n",
      "Iteration 150, loss = 0.00402945\n",
      "Iteration 151, loss = 0.00393096\n",
      "Iteration 152, loss = 0.00397065\n",
      "Iteration 153, loss = 0.00382405\n",
      "Iteration 154, loss = 0.00402750\n",
      "Iteration 155, loss = 0.00374391\n",
      "Iteration 156, loss = 0.00368376\n",
      "Iteration 157, loss = 0.00361125\n",
      "Iteration 158, loss = 0.00361119\n",
      "Iteration 159, loss = 0.00352437\n",
      "Iteration 160, loss = 0.00358667\n",
      "Iteration 161, loss = 0.00338399\n",
      "Iteration 162, loss = 0.00342943\n",
      "Iteration 163, loss = 0.00334983\n",
      "Iteration 164, loss = 0.00328743\n",
      "Iteration 165, loss = 0.00323081\n",
      "Iteration 166, loss = 0.00321185\n",
      "Iteration 167, loss = 0.00315758\n",
      "Iteration 168, loss = 0.00314917\n",
      "Iteration 169, loss = 0.00308208\n",
      "Iteration 170, loss = 0.00303405\n",
      "Iteration 171, loss = 0.00298685\n",
      "Iteration 172, loss = 0.00295794\n",
      "Iteration 173, loss = 0.00298995\n",
      "Iteration 174, loss = 0.00287555\n",
      "Iteration 175, loss = 0.00284866\n",
      "Iteration 176, loss = 0.00285607\n",
      "Iteration 177, loss = 0.00282381\n",
      "Iteration 178, loss = 0.00274091\n",
      "Iteration 179, loss = 0.00272863\n",
      "Iteration 180, loss = 0.00267860\n",
      "Iteration 181, loss = 0.00264991\n",
      "Iteration 182, loss = 0.00265360\n",
      "Iteration 183, loss = 0.00261098\n",
      "Iteration 184, loss = 0.00257692\n",
      "Iteration 185, loss = 0.00256311\n",
      "Iteration 186, loss = 0.00251123\n",
      "Iteration 187, loss = 0.00248025\n",
      "Iteration 188, loss = 0.00244167\n",
      "Iteration 189, loss = 0.00246182\n",
      "Iteration 190, loss = 0.00238395\n",
      "Iteration 191, loss = 0.00237080\n",
      "Iteration 192, loss = 0.00233560\n",
      "Iteration 193, loss = 0.00233313\n",
      "Iteration 194, loss = 0.00230592\n",
      "Iteration 195, loss = 0.00226991\n",
      "Iteration 196, loss = 0.00225952\n",
      "Iteration 197, loss = 0.00222352\n",
      "Iteration 198, loss = 0.00218276\n",
      "Iteration 199, loss = 0.00216985\n",
      "Iteration 200, loss = 0.00214242\n",
      "Iteration 201, loss = 0.00212743\n",
      "Iteration 202, loss = 0.00209393\n",
      "Iteration 203, loss = 0.00208396\n",
      "Iteration 204, loss = 0.00205644\n",
      "Iteration 205, loss = 0.00203530\n",
      "Iteration 206, loss = 0.00202218\n",
      "Iteration 207, loss = 0.00199671\n",
      "Iteration 208, loss = 0.00196783\n",
      "Iteration 209, loss = 0.00196235\n",
      "Iteration 210, loss = 0.00193350\n",
      "Iteration 211, loss = 0.00192737\n",
      "Iteration 212, loss = 0.00191479\n",
      "Iteration 213, loss = 0.00187419\n",
      "Iteration 214, loss = 0.00186615\n",
      "Iteration 215, loss = 0.00184937\n",
      "Iteration 216, loss = 0.00182479\n",
      "Iteration 217, loss = 0.00182707\n",
      "Iteration 218, loss = 0.00180001\n",
      "Iteration 219, loss = 0.00177376\n",
      "Iteration 220, loss = 0.00176943\n",
      "Iteration 221, loss = 0.00173731\n",
      "Iteration 222, loss = 0.00172803\n",
      "Iteration 223, loss = 0.00170429\n",
      "Iteration 224, loss = 0.00168867\n",
      "Iteration 225, loss = 0.00169122\n",
      "Iteration 226, loss = 0.00168106\n",
      "Iteration 227, loss = 0.00164080\n",
      "Iteration 228, loss = 0.00166307\n",
      "Iteration 229, loss = 0.00163103\n",
      "Iteration 230, loss = 0.00165300\n",
      "Iteration 231, loss = 0.00160190\n",
      "Iteration 232, loss = 0.00156222\n",
      "Iteration 233, loss = 0.00157194\n",
      "Iteration 234, loss = 0.00155352\n",
      "Iteration 235, loss = 0.00154236\n",
      "Iteration 236, loss = 0.00152033\n",
      "Iteration 237, loss = 0.00152329\n",
      "Iteration 238, loss = 0.00148376\n",
      "Iteration 239, loss = 0.00148998\n",
      "Iteration 240, loss = 0.00150650\n",
      "Iteration 241, loss = 0.00144939\n",
      "Iteration 242, loss = 0.00145600\n",
      "Iteration 243, loss = 0.00142177\n",
      "Iteration 244, loss = 0.00141431\n",
      "Iteration 245, loss = 0.00141775\n",
      "Iteration 246, loss = 0.00139392\n",
      "Iteration 247, loss = 0.00139624\n",
      "Iteration 248, loss = 0.00136746\n",
      "Iteration 249, loss = 0.00135229\n",
      "Iteration 250, loss = 0.00134212\n",
      "Iteration 251, loss = 0.00133726\n",
      "Iteration 252, loss = 0.00132002\n",
      "Iteration 253, loss = 0.00130760\n",
      "Iteration 254, loss = 0.00130750\n",
      "Iteration 255, loss = 0.00129652\n",
      "Iteration 256, loss = 0.00129402\n",
      "Iteration 257, loss = 0.00127532\n",
      "Iteration 258, loss = 0.00126988\n",
      "Iteration 259, loss = 0.00125759\n",
      "Iteration 260, loss = 0.00124138\n",
      "Iteration 261, loss = 0.00126263\n",
      "Iteration 262, loss = 0.00122672\n",
      "Iteration 263, loss = 0.00120860\n",
      "Iteration 264, loss = 0.00120791\n",
      "Iteration 265, loss = 0.00119976\n",
      "Iteration 266, loss = 0.00117998\n",
      "Iteration 267, loss = 0.00117916\n",
      "Iteration 268, loss = 0.00116392\n",
      "Iteration 269, loss = 0.00116158\n",
      "Iteration 270, loss = 0.00114565\n",
      "Iteration 271, loss = 0.00113250\n",
      "Iteration 272, loss = 0.00112272\n",
      "Iteration 273, loss = 0.00114878\n",
      "Iteration 274, loss = 0.00113547\n",
      "Iteration 275, loss = 0.00111034\n",
      "Iteration 276, loss = 0.00110003\n",
      "Iteration 277, loss = 0.00107836\n",
      "Iteration 278, loss = 0.00108027\n",
      "Iteration 279, loss = 0.00106250\n",
      "Iteration 280, loss = 0.00106392\n",
      "Iteration 281, loss = 0.00105061\n",
      "Iteration 282, loss = 0.00106027\n",
      "Iteration 283, loss = 0.00107502\n",
      "Iteration 284, loss = 0.00103007\n",
      "Iteration 285, loss = 0.00104116\n",
      "Iteration 286, loss = 0.00103256\n",
      "Iteration 287, loss = 0.00100959\n",
      "Iteration 288, loss = 0.00100371\n",
      "Iteration 289, loss = 0.00099338\n",
      "Iteration 290, loss = 0.00098278\n",
      "Iteration 291, loss = 0.00097699\n",
      "Iteration 292, loss = 0.00096953\n",
      "Iteration 293, loss = 0.00096399\n",
      "Iteration 294, loss = 0.00095459\n",
      "Iteration 295, loss = 0.00096851\n",
      "Iteration 296, loss = 0.00094893\n",
      "Iteration 297, loss = 0.00094068\n",
      "Iteration 298, loss = 0.00093708\n",
      "Iteration 299, loss = 0.00093123\n",
      "Iteration 300, loss = 0.00091517\n",
      "Iteration 301, loss = 0.00092796\n",
      "Iteration 302, loss = 0.00091543\n",
      "Iteration 303, loss = 0.00090596\n",
      "Iteration 304, loss = 0.00089840\n",
      "Iteration 305, loss = 0.00089042\n",
      "Iteration 306, loss = 0.00088221\n",
      "Iteration 307, loss = 0.00087466\n",
      "Iteration 308, loss = 0.00086794\n",
      "Iteration 309, loss = 0.00086423\n",
      "Iteration 310, loss = 0.00086038\n",
      "Iteration 311, loss = 0.00085209\n",
      "Iteration 312, loss = 0.00084775\n",
      "Iteration 313, loss = 0.00085263\n",
      "Iteration 314, loss = 0.00083335\n",
      "Iteration 315, loss = 0.00083805\n",
      "Iteration 316, loss = 0.00082254\n",
      "Iteration 317, loss = 0.00082310\n",
      "Iteration 318, loss = 0.00081333\n",
      "Iteration 319, loss = 0.00081237\n",
      "Iteration 320, loss = 0.00080508\n",
      "Iteration 321, loss = 0.00079866\n",
      "Iteration 322, loss = 0.00079491\n",
      "Iteration 323, loss = 0.00079251\n",
      "Iteration 324, loss = 0.00078080\n",
      "Iteration 325, loss = 0.00078367\n",
      "Iteration 326, loss = 0.00077210\n",
      "Iteration 327, loss = 0.00077310\n",
      "Iteration 328, loss = 0.00076462\n",
      "Iteration 329, loss = 0.00076056\n",
      "Iteration 330, loss = 0.00075453\n",
      "Iteration 331, loss = 0.00075918\n",
      "Iteration 332, loss = 0.00075296\n",
      "Iteration 333, loss = 0.00074241\n",
      "Iteration 334, loss = 0.00074685\n",
      "Iteration 335, loss = 0.00073844\n",
      "Iteration 336, loss = 0.00073137\n",
      "Iteration 337, loss = 0.00074546\n",
      "Iteration 338, loss = 0.00072686\n",
      "Iteration 339, loss = 0.00072716\n",
      "Iteration 340, loss = 0.00070966\n",
      "Iteration 341, loss = 0.00071422\n",
      "Iteration 342, loss = 0.00070158\n",
      "Iteration 343, loss = 0.00069942\n",
      "Iteration 344, loss = 0.00069736\n",
      "Iteration 345, loss = 0.00068697\n",
      "Iteration 346, loss = 0.00068724\n",
      "Iteration 347, loss = 0.00068793\n",
      "Iteration 348, loss = 0.00068694\n",
      "Iteration 349, loss = 0.00067262\n",
      "Iteration 350, loss = 0.00067249\n",
      "Iteration 351, loss = 0.00066805\n",
      "Iteration 352, loss = 0.00066489\n",
      "Iteration 353, loss = 0.00065761\n",
      "Iteration 354, loss = 0.00065658\n",
      "Iteration 355, loss = 0.00065368\n",
      "Iteration 356, loss = 0.00064960\n",
      "Iteration 357, loss = 0.00064192\n",
      "Iteration 358, loss = 0.00063893\n",
      "Iteration 359, loss = 0.00063674\n",
      "Iteration 360, loss = 0.00063198\n",
      "Iteration 361, loss = 0.00063230\n",
      "Iteration 362, loss = 0.00062829\n",
      "Iteration 363, loss = 0.00062493\n",
      "Iteration 364, loss = 0.00061790\n",
      "Iteration 365, loss = 0.00061405\n",
      "Iteration 366, loss = 0.00061182\n",
      "Iteration 367, loss = 0.00060751\n",
      "Iteration 368, loss = 0.00060444\n",
      "Iteration 369, loss = 0.00060262\n",
      "Iteration 370, loss = 0.00059801\n",
      "Iteration 371, loss = 0.00059780\n",
      "Iteration 372, loss = 0.00060447\n",
      "Iteration 373, loss = 0.00058620\n",
      "Iteration 374, loss = 0.00059064\n",
      "Iteration 375, loss = 0.00058542\n",
      "Iteration 376, loss = 0.00058558\n",
      "Iteration 377, loss = 0.00057707\n",
      "Iteration 378, loss = 0.00057512\n",
      "Iteration 379, loss = 0.00056982\n",
      "Iteration 380, loss = 0.00056967\n",
      "Iteration 381, loss = 0.00056449\n",
      "Iteration 382, loss = 0.00056096\n",
      "Iteration 383, loss = 0.00055911\n",
      "Iteration 384, loss = 0.00055679\n",
      "Iteration 385, loss = 0.00055176\n",
      "Iteration 386, loss = 0.00054906\n",
      "Iteration 387, loss = 0.00055035\n",
      "Iteration 388, loss = 0.00054329\n",
      "Iteration 389, loss = 0.00054092\n",
      "Iteration 390, loss = 0.00053970\n",
      "Iteration 391, loss = 0.00053681\n",
      "Iteration 392, loss = 0.00053564\n",
      "Iteration 393, loss = 0.00053225\n",
      "Iteration 394, loss = 0.00052747\n",
      "Iteration 395, loss = 0.00052466\n",
      "Iteration 396, loss = 0.00052205\n",
      "Iteration 397, loss = 0.00052790\n",
      "Iteration 398, loss = 0.00051545\n",
      "Iteration 399, loss = 0.00051634\n",
      "Iteration 400, loss = 0.00051480\n",
      "Iteration 401, loss = 0.00050827\n",
      "Iteration 402, loss = 0.00050705\n",
      "Iteration 403, loss = 0.00050436\n",
      "Iteration 404, loss = 0.00050291\n",
      "Iteration 405, loss = 0.00050695\n",
      "Iteration 406, loss = 0.00049722\n",
      "Iteration 407, loss = 0.00049839\n",
      "Iteration 408, loss = 0.00049273\n",
      "Iteration 409, loss = 0.00050261\n",
      "Iteration 410, loss = 0.00048650\n",
      "Iteration 411, loss = 0.00049278\n",
      "Iteration 412, loss = 0.00048632\n",
      "Iteration 413, loss = 0.00048611\n",
      "Iteration 414, loss = 0.00047592\n",
      "Iteration 415, loss = 0.00047928\n",
      "Iteration 416, loss = 0.00048105\n",
      "Iteration 417, loss = 0.00047169\n",
      "Iteration 418, loss = 0.00046902\n",
      "Iteration 419, loss = 0.00046593\n",
      "Iteration 420, loss = 0.00046443\n",
      "Iteration 421, loss = 0.00046495\n",
      "Iteration 422, loss = 0.00046020\n",
      "Iteration 423, loss = 0.00045814\n",
      "Iteration 424, loss = 0.00045550\n",
      "Iteration 425, loss = 0.00045374\n",
      "Iteration 426, loss = 0.00045116\n",
      "Iteration 427, loss = 0.00044838\n",
      "Iteration 428, loss = 0.00044638\n",
      "Iteration 429, loss = 0.00044515\n",
      "Iteration 430, loss = 0.00044240\n",
      "Iteration 431, loss = 0.00043977\n",
      "Iteration 432, loss = 0.00043949\n",
      "Iteration 433, loss = 0.00043948\n",
      "Iteration 434, loss = 0.00043926\n",
      "Iteration 435, loss = 0.00043456\n",
      "Iteration 436, loss = 0.00043465\n",
      "Iteration 437, loss = 0.00042990\n",
      "Iteration 438, loss = 0.00042668\n",
      "Iteration 439, loss = 0.00042873\n",
      "Iteration 440, loss = 0.00042457\n",
      "Iteration 441, loss = 0.00042242\n",
      "Iteration 442, loss = 0.00042048\n",
      "Iteration 443, loss = 0.00041744\n",
      "Iteration 444, loss = 0.00042275\n",
      "Iteration 445, loss = 0.00041443\n",
      "Iteration 446, loss = 0.00041445\n",
      "Iteration 447, loss = 0.00041117\n",
      "Iteration 448, loss = 0.00040931\n",
      "Iteration 449, loss = 0.00040630\n",
      "Iteration 450, loss = 0.00040416\n",
      "Iteration 451, loss = 0.00040291\n",
      "Iteration 452, loss = 0.00040114\n",
      "Iteration 453, loss = 0.00040237\n",
      "Iteration 454, loss = 0.00040011\n",
      "Iteration 455, loss = 0.00040039\n",
      "Iteration 456, loss = 0.00039592\n",
      "Iteration 457, loss = 0.00039396\n",
      "Iteration 458, loss = 0.00039058\n",
      "Iteration 459, loss = 0.00038995\n",
      "Iteration 460, loss = 0.00038806\n",
      "Iteration 461, loss = 0.00039173\n",
      "Iteration 462, loss = 0.00038418\n",
      "Iteration 463, loss = 0.00038275\n",
      "Iteration 464, loss = 0.00038195\n",
      "Iteration 465, loss = 0.00038090\n",
      "Iteration 466, loss = 0.00037894\n",
      "Iteration 467, loss = 0.00037707\n",
      "Iteration 468, loss = 0.00037570\n",
      "Iteration 469, loss = 0.00037304\n",
      "Iteration 470, loss = 0.00037169\n",
      "Iteration 471, loss = 0.00037198\n",
      "Iteration 472, loss = 0.00036821\n",
      "Iteration 473, loss = 0.00036803\n",
      "Iteration 474, loss = 0.00036610\n",
      "Iteration 475, loss = 0.00036636\n",
      "Iteration 476, loss = 0.00036161\n",
      "Iteration 477, loss = 0.00036246\n",
      "Iteration 478, loss = 0.00036016\n",
      "Iteration 479, loss = 0.00035892\n",
      "Iteration 480, loss = 0.00035752\n",
      "Iteration 481, loss = 0.00035529\n",
      "Iteration 482, loss = 0.00035514\n",
      "Iteration 483, loss = 0.00035442\n",
      "Iteration 484, loss = 0.00035178\n",
      "Iteration 485, loss = 0.00034992\n",
      "Iteration 486, loss = 0.00034883\n",
      "Iteration 487, loss = 0.00034892\n",
      "Iteration 488, loss = 0.00034905\n",
      "Iteration 489, loss = 0.00034443\n",
      "Iteration 490, loss = 0.00034294\n",
      "Iteration 491, loss = 0.00034185\n",
      "Iteration 492, loss = 0.00034071\n",
      "Iteration 493, loss = 0.00033922\n",
      "Iteration 494, loss = 0.00033843\n",
      "Iteration 495, loss = 0.00033603\n",
      "Iteration 496, loss = 0.00033502\n",
      "Iteration 497, loss = 0.00033398\n",
      "Iteration 498, loss = 0.00033254\n",
      "Iteration 499, loss = 0.00033110\n",
      "Iteration 500, loss = 0.00032910\n",
      "Iteration 501, loss = 0.00032776\n",
      "Iteration 502, loss = 0.00032528\n",
      "Iteration 503, loss = 0.00032298\n",
      "Iteration 504, loss = 0.00031983\n",
      "Iteration 505, loss = 0.00031892\n",
      "Iteration 506, loss = 0.00031258\n",
      "Iteration 507, loss = 0.00030903\n",
      "Iteration 508, loss = 0.00030652\n",
      "Iteration 509, loss = 0.00030292\n",
      "Iteration 510, loss = 0.00030115\n",
      "Iteration 511, loss = 0.00029679\n",
      "Iteration 512, loss = 0.00029398\n",
      "Iteration 513, loss = 0.00029022\n",
      "Iteration 514, loss = 0.00028963\n",
      "Iteration 515, loss = 0.00028812\n",
      "Iteration 516, loss = 0.00028410\n",
      "Iteration 517, loss = 0.00028342\n",
      "Iteration 518, loss = 0.00028017\n",
      "Iteration 519, loss = 0.00027832\n",
      "Iteration 520, loss = 0.00027702\n",
      "Iteration 521, loss = 0.00027557\n",
      "Iteration 522, loss = 0.00027224\n",
      "Iteration 523, loss = 0.00027214\n",
      "Iteration 524, loss = 0.00026852\n",
      "Iteration 525, loss = 0.00026833\n",
      "Iteration 526, loss = 0.00026579\n",
      "Iteration 527, loss = 0.00026321\n",
      "Iteration 528, loss = 0.00026454\n",
      "Iteration 529, loss = 0.00026199\n",
      "Iteration 530, loss = 0.00026043\n",
      "Iteration 531, loss = 0.00025899\n",
      "Iteration 532, loss = 0.00025880\n",
      "Iteration 533, loss = 0.00025638\n",
      "Iteration 534, loss = 0.00025448\n",
      "Iteration 535, loss = 0.00025395\n",
      "Iteration 536, loss = 0.00025011\n",
      "Iteration 537, loss = 0.00024769\n",
      "Iteration 538, loss = 0.00024757\n",
      "Iteration 539, loss = 0.00024318\n",
      "Iteration 540, loss = 0.00024173\n",
      "Iteration 541, loss = 0.00024228\n",
      "Iteration 542, loss = 0.00024496\n",
      "Iteration 543, loss = 0.00024498\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "nnmlpadam = mlpadam.fit(trainData,trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnmlplbfgs = mlplbfgs.fit(trainData,trainLabelE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub15' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-81f493155709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m221\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstripplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub15\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstripplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub15\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub15' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAJyCAYAAADHFlH/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/R+WCoI6Pyk2HgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110ddb128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "fig = plt.figure(figsize=(32,24))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1 = sb.stripplot(x='Activity', y=sub15.iloc[:2], data=sub15, jitter = True)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2 = sb.stripplot(x='Activity', y=sub15.iloc[:3], data=sub15, jitter = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData,trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86986986986986992"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(testData,testLabelE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression score: 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
